{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQL Schema from CSV Files \n",
    "#### Joe Eberle, Alan Calhoun, Al Seoud\n",
    "##### 9/20/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c283fb608226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mData_Import_Starting_Directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Y:/_Kaleida_Input/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mProcess_Name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Importing CSV data into SQL'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtalking_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mcode_speaking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mevent_log_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "#install dependent Libraries is not already installed \n",
    "#!pip install pyttsx3\n",
    "\n",
    "# Import the necessary Libraries \n",
    "import glob, os\n",
    "import pandas as pd\n",
    "# import logging \n",
    "from pathlib import Path\n",
    "import pyttsx3\n",
    "import pyodbc \n",
    "import timeit\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sqlalchemy\n",
    "\n",
    "# Establish some import parameters \n",
    "\n",
    "importing_xlsx_files = False \n",
    "importing_csv_files = True \n",
    "Data_Import_Starting_Directory = 'Y:/_Kaleida_Input/'\n",
    "Data_Import_Starting_Directory = 'C:/Data/'\n",
    "Process_Name = 'Importing CSV data into SQL'\n",
    "talking_code = False \n",
    "\n",
    "event_log_row = 0 \n",
    "\n",
    "# Create some Global Variables for SQL Constructs \n",
    "column_inserts = ''\n",
    "column_question_mark   = ''\n",
    "create_table_SQL  = ''\n",
    "create_real_table_SQL  = ''\n",
    "insert_records_SQL  = ''\n",
    "create_schema_SQL  = '' \n",
    "create_real_schema_SQL  = '' \n",
    "Table_Name_Extension_Daily = '_DI'\n",
    "Table_Name_Extension_Historical = '_HX'\n",
    "Table_Name_Extension_Administrative = '_AD'\n",
    "Table_Name_Prefix = '[pbic_1_0].'\n",
    "\n",
    "# Create some Global Variables for SQL Connection\n",
    "server = 'Kalpwvsqlgppc01' \n",
    "database  = 'GPPC_DEV' \n",
    "username ='GPPC'\n",
    "pwd = 'Elephant-Trunk-06'\n",
    "sql_connector = 'DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd\n",
    "# log_filename='data_importing_log.log'\n",
    "\n",
    "# Configure the Logging to the DEBUG Level \n",
    "# logging.basicConfig(level=logging.DEBUG, filename=log_filename, format= '%(asctime)s %(clientip)-15s %(user)-8s %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Configuration and SETUP \n",
    "### Before running this notebook :\n",
    "- Go to bottom of code and build the following functions:\n",
    "--    initialize_replacement_Dictionaries()    # Set up control libraries for syntactic Consistency \n",
    "--    Initialize_Text_to_Speach()              # Intitialize Text to Speech Engine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_python_infrastructure():\n",
    "    initialize_replacement_Dictionaries()    # Set up control libraries for syntactic Consistency \n",
    "    Initialize_Text_to_Speach()              # Intitialize Text to Speech Engine \n",
    "    df_e_log = create_event_log_dataframe()  # Set up the Event Logging \n",
    "    \n",
    " \n",
    "Text_to_Speech = pyttsx3.init() \n",
    "\n",
    "# Intitialize Text to Speech Engine \n",
    "def Initialize_Text_to_Speach():\n",
    "    Text_to_Speech = pyttsx3.init()\n",
    "    Text_to_Speech.setProperty('rate',187)\n",
    "    voices = Text_to_Speech.getProperty('voices')\n",
    "    Text_to_Speech.setProperty('voice', voices[0].id)\n",
    "    speech = 'The text to speech engine is initialized using pythons pyttsx3 engine'\n",
    "    Text_to_Speech.say(speech)\n",
    "    Text_to_Speech.runAndWait()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Say Whatever the user wants \n",
    "def say(speech):\n",
    "    Text_to_Speech.say(speech)\n",
    "    Text_to_Speech.runAndWait()    \n",
    " \n",
    "\n",
    "# Set up control libraries for syntactic Consistency \n",
    "def initialize_replacement_Dictionaries(): \n",
    "\n",
    "## Dictionary For character_replacements List \n",
    "    character_replacements = { \" \":\"_\",\"#\":\"Number\",\"#\":\"Number\",\"%\":\"Percentage\" \\\n",
    "                             ,'_Unnamed':'','_Level':'',\"$\":\"Dollar\",'_1':'' \\\n",
    "                             ,'_2':'','_3':'','_4':'','_5':''  \\\n",
    "                             ,'_6':'','_7':'','_8':'','_9':''  \\\n",
    "                             ,'_0':'',':7':'',':8':'',':':'' }\n",
    "## Dictionary For replacing data types in databases \n",
    "    data_type_replacements = { \"object\":\"varchar\",\"float64\":\"float\",\"int64\":\"int\",\"%\":\"Percentage\" \\\n",
    "                             ,'_Unnamed':'','datetime64':'timestamp',\"timedelta64[ns]\":\"varcahr\"}    \n",
    "    \n",
    "# Create dataframe to house Directories \n",
    "def create_directory_dataframe():\n",
    "    df_import_directories = pd.DataFrame(columns = ('Root_Directory','Sub_Directory','Table_Name','Import_File_Name'))\n",
    "    return df_import_directories  \n",
    "df_Import_Files = create_directory_dataframe() \n",
    "\n",
    "def add_log_event(Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments ):\n",
    "    global event_log_row  \n",
    "    event_log_row += 1  \n",
    "    df_e_log.loc[event_log_row] = [event_log_row,Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments]\n",
    " \n",
    "\n",
    "# Create dataframe to house Directories \n",
    "def create_event_log_dataframe():\n",
    "    global event_log_row  \n",
    "    event_log_row = 0 \n",
    "    df_event_log = pd.DataFrame(columns = ('Event_ID','Process_Name','Event_Name','Event_Date','Event_Time','Task_Start_Time','Task_End_Time','TASk_Duration','Comments'))\n",
    "    return df_event_log\n",
    "\n",
    "\n",
    "df_e_log = create_event_log_dataframe()\n",
    "\n",
    "# Create dataframe to database schema \n",
    "def create_database_schema_dataframe():\n",
    "    df_schema = pd.DataFrame(columns = ('Database_Name','Table_Name','Column_Number','Column_Name','Column_Data_Type','Column_Sample_Data','Column_Description'))\n",
    "    return df_schema\n",
    "\n",
    "# Create dataframe to house Directories \n",
    "def add_log_event(Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments ):\n",
    "    global event_log_row  \n",
    "    event_log_row += 1  \n",
    "    df_e_log.loc[event_log_row] = [event_log_row,Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments]\n",
    "    \n",
    "    # Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    \n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "        \n",
    "        \n",
    "# Introduction - Overview of CSV to SQL Import Process Steps \n",
    "def read_credits(): \n",
    "    Dialog = 'This Jupyter Notebook Was  : '\n",
    "    Dialog = Dialog + 'Developed in Collaboration by Joe Eberle, Alan Calhoun, Al Seoud  '\n",
    "    Dialog = Dialog + 'Developed in Python starting on 9/20/2022 '\n",
    "    Dialog = Dialog + 'This package is free AND Open Source and THE CODE IS openly available for general Use. '    \n",
    "    say(Dialog)         \n",
    "    \n",
    "# Introduction - Overview of CSV to SQL Import Process Steps \n",
    "def read_terms(): \n",
    "    Dialog = 'The terminology for this process is : '\n",
    "    Dialog = Dialog + 'Python. Python is a general-purpose programming language that is widely used for data science.  '\n",
    "    Dialog = Dialog + 'Structured Query Language (SQL) is one of the worlds most widely used programming languages for manipulating and querying data. '\n",
    "    Dialog = Dialog + 'CSV. A Comma-Separated Values (CSV)  file is a text file in which information is separated by commas. '\n",
    "    Dialog = Dialog + 'PANDAS. Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.  '\n",
    "    Dialog = Dialog + 'OS PACKAGE - The OS python library provides a portable way of using operating system dependent functionality to allow your python code to run on all platforms '\n",
    "    say(Dialog)  \n",
    "    \n",
    "# Process Steps - Overview of CSV to SQL Import Process Steps \n",
    "def read_process_steps():\n",
    "    Dialog = 'The data flow for this process is : '\n",
    "    Dialog = Dialog + 'Precursor Step 1: The clinician or administrator enters the patients data into the Electronic Medical Record (EMR). '\n",
    "    Dialog = Dialog + 'Precursor Step 2: At the end of the day the EMR data is exported into Comma Seperated Values (CSV) files and shared via SFTP. '\n",
    "    Dialog = Dialog + 'Step 1: Identify the location of the CSV file root directory. '\n",
    "    Dialog = Dialog + 'Step 2: Walk the directory structure discovering data to import in the form of CSV Files '\n",
    "    Dialog = Dialog + 'Step 3: Read the CSV code into python PANDAS. '\n",
    "    Dialog = Dialog + 'Step 7: Clean the data and makes it consistent in PANDAS. ' \n",
    "    Dialog = Dialog + 'Step 5: Check the consistency of the data perform change control ' \n",
    "    Dialog = Dialog + 'Step 6: Convert the pandas dataframes into SQL table Create Statements  '\n",
    "    Dialog = Dialog + 'Step 7: This python code then Creates the SQL tables in the target Database   '\n",
    "    Dialog = Dialog + 'Step 8: Insert the the PANDAS Rows into SQL using the to_SQL Method.    '\n",
    "    Dialog = Dialog + 'Optional Step 8b: Optionally also import the PANDAS rows into SNOW FLAKE  using the SNOW FLAKE Input Method.    '\n",
    "    Dialog = Dialog + 'Step 9: This python code uses event logging to capture the performance of the entire process.    '\n",
    "    Dialog = Dialog + 'Step 10: This python code documents the table structures to capture the entire SCHEMA into an easy to use Excel Spreadsheet.    '\n",
    "    Dialog = Dialog + 'Step 11: This Python code checks the total Number of records imported to the total raw record count to make sure no data is Left Behind    '\n",
    "\n",
    "    say(Dialog)    \n",
    "    \n",
    "    \n",
    "# Introduction - Overview of NoteBooks  \n",
    "def read_introduction():\n",
    "    Dialog = 'This jupyter notebook will import all of the CSV files under a specific root directory into a database. '\n",
    "    Dialog = Dialog +  'This python code will take the CSV files exported froms an Electronic Medical Record platform. '\n",
    "    Dialog = Dialog + 'and import them into a faster database such as PostgreSQL or SQL Server or SNOW Flake. '\n",
    "    Dialog = Dialog + 'the data is then available for anaylsis using query tools or ready for visualizations in Power BI or Tableau. '\n",
    "    say(Dialog)    \n",
    "    \n",
    "\n",
    "def set_up_python_infrastructure():\n",
    "    initialize_replacement_Dictionaries()    # Set up control libraries for syntactic Consistency \n",
    "    Initialize_Text_to_Speach()              # Intitialize Text to Speech Engine \n",
    "    df_e_log = create_event_log_dataframe()  # Set up the Event Logging to housae the events of this process \n",
    "    create_database_schema_dataframe()       # Set up the Database Schema dataframe to house the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify the location of the CSV file root directory. \n",
    "## Step 2: Walk the directory structure discovering data to import in the form of CSV Files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_sub_directories for root Directory C:/Data/ \n",
      "\n",
      "Directory entry# 1 C:/Data/  \n",
      "Directory entry# 2 C:/Data/Behavioral Health  \n",
      "Directory entry# 3 C:/Data/brfss_cdc  \n",
      "Directory entry# 4 C:/Data/Chronic Kidney Disease  \n",
      "Directory entry# 5 C:/Data/Chronic_Disease_Indicators  \n",
      "Directory entry# 6 C:/Data/Data_Sciance_Data  \n",
      "Directory entry# 7 C:/Data/Data_Sciance_Data\\Projects  \n",
      "Directory entry# 8 C:/Data/Data_Sciance_Data\\Projects\\AIHS  \n",
      "Directory entry# 9 C:/Data/Data_Sciance_Data\\Projects\\AIHS\\Architecture  \n",
      "Directory entry# 10 C:/Data/Data_Sciance_Data\\Projects\\AIHS\\Chatbot Initial Conversation - Botsociety_files  \n",
      "Directory entry# 11 C:/Data/Data_Sciance_Data\\Test_Data  \n",
      "Directory entry# 12 C:/Data/Data_Sciance_Data\\Test_Data\\India Health Stats  \n",
      "Directory entry# 13 C:/Data/Data_Sciance_Data\\Test_Data\\Medicare  \n",
      "Directory entry# 14 C:/Data/Data_Sciance_Data\\Test_Data\\Mental_Health_FAQ_for_Chatbot  \n",
      "Directory entry# 15 C:/Data/Data_Sciance_Data\\Test_Data\\New_Kaggle_Data  \n",
      "Directory entry# 16 C:/Data/Data_Sciance_Data\\Test_Data\\nhanes  \n",
      "Directory entry# 17 C:/Data/Diabetes  \n",
      "Directory entry# 18 C:/Data/Heart  \n",
      "End of function \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root_Directory</th>\n",
       "      <th>Sub_Directory</th>\n",
       "      <th>Table_Name</th>\n",
       "      <th>Import_File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Root_Directory, Sub_Directory, Table_Name, Import_File_Name]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory):\n",
    "    df_import_directories = create_directory_dataframe() \n",
    "    print('list_sub_directories for root Directory {} \\n'.format(root_directory) )   \n",
    "    directory_entry = 0 \n",
    "    files_to_import = 0 \n",
    "    Table_Name = ''\n",
    "    for root, subdirectories, files in os.walk(root_directory):\n",
    "        directory_entry += 1 \n",
    "        print('Directory entry# {} {}  '.format(directory_entry,root ))\n",
    "        #print('Root: {} has subdirs{}'.format(root, subdirectories)\n",
    "              \n",
    "    print('End of function ')              \n",
    "    return df_import_directories\n",
    "              \n",
    "walk_sub_directories('C:/Data/')              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up_python_infrastructure()\n",
    "read_terms()\n",
    "Initialize_Text_to_Speach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_introduction()\n",
    "read_credits() \n",
    "read_process_steps()\n",
    "read_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>Process_Name</th>\n",
       "      <th>Event_Name</th>\n",
       "      <th>Event_Date</th>\n",
       "      <th>Event_Time</th>\n",
       "      <th>Task_Start_Time</th>\n",
       "      <th>Task_End_Time</th>\n",
       "      <th>TASk_Duration</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>Starting Import Process</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-09-30 16:00:57.141395</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>3.000133</td>\n",
       "      <td>Starting Import Process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>Reading the CSV files</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-09-30 16:00:59.206324</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>2.000181</td>\n",
       "      <td>Reading the CSV filesS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>Writing to SQL Server</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-09-30 16:01:00.221022</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>1.000714</td>\n",
       "      <td>Writing to SQL Server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>Import Process END</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-09-30 16:01:02.224364</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>1.664568e+09</td>\n",
       "      <td>2.000351</td>\n",
       "      <td>Import Process END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Event_ID                 Process_Name               Event_Name  Event_Date  \\\n",
       "1        1  Importing CSV data into SQL  Starting Import Process  2022-09-30   \n",
       "2        2  Importing CSV data into SQL    Reading the CSV files  2022-09-30   \n",
       "3        3  Importing CSV data into SQL    Writing to SQL Server  2022-09-30   \n",
       "4        4  Importing CSV data into SQL      Import Process END   2022-09-30   \n",
       "\n",
       "                  Event_Time  Task_Start_Time  Task_End_Time  TASk_Duration  \\\n",
       "1 2022-09-30 16:00:57.141395     1.664568e+09   1.664568e+09       3.000133   \n",
       "2 2022-09-30 16:00:59.206324     1.664568e+09   1.664568e+09       2.000181   \n",
       "3 2022-09-30 16:01:00.221022     1.664568e+09   1.664568e+09       1.000714   \n",
       "4 2022-09-30 16:01:02.224364     1.664568e+09   1.664568e+09       2.000351   \n",
       "\n",
       "                  Comments  \n",
       "1  Starting Import Process  \n",
       "2   Reading the CSV filesS  \n",
       "3    Writing to SQL Server  \n",
       "4       Import Process END  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "global Process_Name \n",
    "\n",
    "today = date.today()\n",
    "\n",
    "start_time1 = time.time() \n",
    "time.sleep(3)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Starting Import Process',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Starting Import Process\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Reading the CSV files',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Reading the CSV filesS\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(1)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Writing to SQL Server',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Writing to SQL Server\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Import Process END ',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Import Process END\")\n",
    "\n",
    "df_e_log.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_create_SQL (import_df):\n",
    "    column_name_List = [x.title() for x in import_df.columns] # Create a List of Columns \n",
    "    column_Str =  (', '.join(column_name_List)) # Convert List into one String with commas \n",
    "   # print('Columns =',column_Str)  \n",
    "    return column_Str\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_datatype_Str = str(df_e_log.dtypes)\n",
    "column_datatype_Str =  column_datatype_Str.replace('dtype: object','').replace('object','varchar[255], ').replace('datetime64[ns]','timestamp, ').replace('float64','float, ')\n",
    "print('create column SQL string:\\n', column_datatype_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e_log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_columns = column_create_SQL(df_e_log)\n",
    "sql_columns_cleaned = ' '.join([character_replacements.get(i, i) for i in sql_columns.split()])\n",
    "sql_column_data_types_cleaned = ' '.join([data_type_replacements.get(i, i) for i in sql_column_data_types.split()])\n",
    "\n",
    "print('SQL Columns  =',sql_columns ,' /n ' 'SQL Columns Cleaned =',sql_columns_cleaned) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    \n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "list_all_csv_files('Y:/_Kaleida_Input/Access/')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_csv_files(path):\n",
    "    \n",
    "    if importing_xlsx_files: \n",
    "        extension = 'xlsx'\n",
    "    if importing_csv_files: \n",
    "        extension = 'csv'        \n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "list_all_csv_files('Y:/_Kaleida_Input/Available_Slots/')  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_table_name_for_path(path):\n",
    "\n",
    "    table_name = path.replace(Data_Import_Starting_Directory,\"\").replace(' ','_').replace('/','').replace('\\\\','')\n",
    "    return table_name \n",
    "\n",
    "\n",
    "path = 'Y:/_Kaleida_Input/Available_Slots/'\n",
    "table_name_for_path = determine_table_name_for_path(path)\n",
    "print('Table Name:{} is determined from path:{}'.format(table_name_for_path,path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_import_all_csv_files(path):\n",
    "    \n",
    "    if importing_xlsx_files: \n",
    "        extension = 'xlsx'\n",
    "    if importing_csv_files: \n",
    "        extension = 'csv'        \n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "read_and_import_all_csv_files('Y:/_Kaleida_Input/Available_Slots/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def list_all_subdirectories(path):\n",
    "    \n",
    "    os.chdir(path)\n",
    "    print('Listing Directories Under Directory: {}'.format(path))\n",
    "    sub_directory_count = 0\n",
    "    file_count = 0    \n",
    "    for file in os.listdir(path):\n",
    "        print('Dir {} file {} '.format(str(sub_directory_count),file))\n",
    "        if os.path.isdir(file): \n",
    "            sub_directory_count += 1 \n",
    "            list_all_subdirectories(path + file)\n",
    "        elif os.path.isfile(file)  and file.find('csv') > 1  :\n",
    "            file_count += 1      \n",
    "    print('Traversed {} subdirectories and found {} files '.format(str(sub_directory_count), str(file_count)))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Iport Walk at Directory:',Data_Import_Starting_Directory)\n",
    "list_all_subdirectories(Data_Import_Starting_Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def execute_table_Create_SQL(path, table_name):\n",
    "    \n",
    "    executing_SQL = False \n",
    "    \n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        if csv_file_count == 1:\n",
    "            print('Creating_Table',table_name,' based upon 1st sample','File',str(csv_file_count),\": \", file)     \n",
    "            data_folder =  path\n",
    "            filename = data_folder + table_name +'.csv'\n",
    "            print ('raw file name to rcreate from:',filename)\n",
    "            DROP_table_SQL = build_DROP_table_SQL(file, table_name,'_DI]')  \n",
    "            create_table_SQL = build_table_create_SQL(file, table_name,'_DI]')\n",
    "\n",
    "    if executing_SQL:     \n",
    "            print ('/n DROP SQL = ',DROP_table_SQL  )         \n",
    "            print ('/n create SQL = ',create_table_SQL  )     \n",
    "            execute_SQL(DROP_table_SQL)            \n",
    "            execute_SQL(create_table_SQL)\n",
    "        \n",
    "execute_table_Create_SQL('Y:/_Kaleida_Input/Access/','Access')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory): \n",
    " #print('list_sub_directories for root Ditrectory {} \\n'.format(root_directory) )   \n",
    " directory_entry = 0 \n",
    " Table_Name = ''\n",
    " print('About to Walk')\n",
    " for root,directory, file in os.walk(root_directory):\n",
    "    print('Walking....   ')\n",
    "    #print('Root Directory: {} subdir: {} \\n'.format(root_directory,directory_entry) )\n",
    "    \n",
    "    if root.find('Access') >= 0:\n",
    "        Table_Name = 'Access'\n",
    "    elif root.find('Daily Time Card') >= 0:\n",
    "        Table_Name = 'Daily_Time_Card'                \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'               \n",
    "    elif root.find('ADP') >= 0:\n",
    "        Table_Name = 'ADP'  \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'   \n",
    "    elif root.find('Available_Slots') >= 0:\n",
    "        Table_Name = 'Available_Slots'     \n",
    "    elif root.find('Available_Slots_Past') >= 0:\n",
    "        Table_Name = 'Available_Slots_Past'    \n",
    "    elif root.find('Call Center') >= 0:\n",
    "        Table_Name = 'Call_Center'    \n",
    "    elif root.find('CPT Visit') >= 0:\n",
    "        Table_Name = 'CPT_Visit'    \n",
    "    elif root.find('Visit') >= 0:\n",
    "        Table_Name = 'Visit'  \n",
    "    elif root.find('DailyAppointments') >= 0:\n",
    "        Table_Name = 'Daily_Appointments'    \n",
    "    elif root.find('DailyCPT') >= 0:\n",
    "        Table_Name = 'Daily_CPT'    \n",
    "    elif root.find('DailyMultipleAppointmentSameDay') >= 0:\n",
    "        Table_Name = 'Daily_Multiple_Appointment_Same_Day'   \n",
    "    elif root.find('DailyScheduledOfficeAppointmentVisit') >= 0:\n",
    "        Table_Name = 'Daily_Scheduled_Office_Appointment_Visit'               \n",
    "        PatientExperienceDefault\n",
    "    else:\n",
    "        Table_Name = 'Unknown Table Name'\n",
    "    print('Root Directory'+str(directory_entry) +':',root+'Default Table Name for Directory :', Table_Name,' \\n')            \n",
    "    #list_all_csv_files(root)  \n",
    "    if Table_Name != 'Unknown Table Name':\n",
    "        execute_table_Create_SQL(root,Table_Name)  \n",
    "    directory_entry += 1     \n",
    " \n",
    "# Test function call     \n",
    "list_sub_directories('Y:/_Kaleida_Input/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP the table Dynamically \n",
    "def drop_table_SQL(drop_SQL):\n",
    "    global sql_connector\n",
    "    print('Drop Table - Before SQL Connect - Call')\n",
    " #   logging.debug('Drop Table - Before SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(drop_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SQL  Dynamically \n",
    "def execute_SQL(execute_SQL_command):\n",
    "    global sql_connector\n",
    "    print('Execute SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(execute_SQL_command)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Drop_Table_SQL(Table_Name):\n",
    "    #drop_SQL = 'DROP TABLE [pbic_1_0].[' + Table_Name + ']'\n",
    "    \n",
    "    \n",
    "    drop_SQL =  'DROP TABLE [pbic_1_0].[{}]'.format(\"'\", Table_Name)\n",
    " \n",
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Access_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Access_DI]    \n",
    "    return drop_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Table_SQL = Create_Drop_Table_SQL('Access_DI')\n",
    "\n",
    "print(Drop_Table_SQL) \n",
    "#execute_SQL(Drop_Table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_file(data_folder, filename):\n",
    "\n",
    "    print('Import File =', filename)                 \n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    #print(df_input_csv.columns)\n",
    "    df_input_csv.columns = df_input_csv.columns.map('^'.join)\n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "\n",
    "    print(df_input_csv.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table_create_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    "    print('Import File =', filename)   \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    \n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"Address 1\",\"Street_Address\").replace(\"Address 2\",\"Address_Two\") \\\n",
    "                             .replace(\"^\",\"\").replace(\"-\",\"_\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\") \\\n",
    "                             .replace(\"%\",\"Percentage\").replace('_Unnamed','').replace('Unnamed','') \\\n",
    "                             .replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "    \n",
    "         \n",
    "            \n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Dataframe_table_create_SQL(dataframe_to_Create, table_name, table_Postfix  ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    " \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    \n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"Address 1\",\"Street_Address\").replace(\"Address 2\",\"Address_Two\") \\\n",
    "                             .replace(\"^\",\"\").replace(\"-\",\"_\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\") \\\n",
    "                             .replace(\"%\",\"Percentage\").replace('_Unnamed','').replace('Unnamed','') \\\n",
    "                             .replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "    \n",
    "         \n",
    "            \n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DROP_table_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    DROP_table_SQL  = 'DROP Table ' + Table_Schema_Prefix + Table_Name  \n",
    "    return DROP_table_SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "\n",
    "print(\"\\nDaily Table Create SQL: \\n\" + create_table_SQL)\n",
    "\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_HX]')\n",
    "\n",
    "print(\"\\nHistorical Table Create SQL: \\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y:/_Kaleida_Input/Access/2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments')\n",
    "\n",
    "print(\"\\n\" + create_table_SQL)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Daily_Appointments_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Daily_Appointments_DI]\n",
    "\n",
    "\n",
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_SQL(create_table_SQL)\n",
    "print(\"\\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "# filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    " \n",
    "read_and_clean_file(data_folder,filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_table_headers(input_data_frame):\n",
    "    global column_inserts\n",
    "    global column_question_mark\n",
    "    global create_table_SQL\n",
    "    global create_real_table_SQL\n",
    "    global insert_records_SQL\n",
    "    global Table_Name_Prefix\n",
    "    global Table_Name_Extension_Daily\n",
    "    global Table_Name_Extension_Historical\n",
    "    global create_schema_SQL\n",
    "    global create_real_schema_SQL\n",
    "        \n",
    "    Table_Name = 'Access'\n",
    "    sample_row = 3\n",
    "    create_table_SQL = ''\n",
    "    create_real_table_SQL = ''\n",
    "    insert_records_SQL = ''\n",
    "    Table_Name_Daily = Table_Name_Prefix + Table_Name + Table_Name_Extension_Daily\n",
    "    Table_Name_Historical = Table_Name_Prefix + Table_Name + Table_Name_Extension_Historical\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes\n",
    "    col_number = 0\n",
    "    column_creates = ''\n",
    "    column_values = ''\n",
    "    column_inserts = ''\n",
    "    real_column_creates = ''\n",
    "    column_question_mark = ''\n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            real_sql_column_type = 'Varchar(255)'                \n",
    "                \n",
    "\n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name\n",
    "        column_value = str(input_data_frame.iloc[sample_row,col_number-1])\n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type\n",
    "        real_column_creates = real_column_creates + column_name + \" \"  + real_sql_column_type\n",
    "        column_values = column_values + \"'\" + column_value + \"'\"\n",
    "        print(col_number, '  ', column_name)\n",
    "            \n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name_Daily + '  (' + column_inserts + ') + VALUES (' + column_values + '); '\n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name_Daily + '  (' + column_creates + '); '\n",
    "    create_real_table_SQL = 'CREATE TABLE ' + Table_Name_Historical + '  (' + real_column_creates + '); '\n",
    "    create_schema_SQL = create_schema_SQL + create_table_SQL\n",
    "    create_real_schema_SQL = create_real_schema_SQL + create_real_table_SQL\n",
    "    #logging.debug('Table Create Finished')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'Y:\\_Kaleida_Input\\Access\\2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "start_time1 = time.time()\n",
    "\n",
    "data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "print('Import File =', filename)                 \n",
    "df_input_csv = pd.read_csv(filename, nrows=10, header=[0,1])\n",
    "df_input_csv.columns = df_input_csv.columns.map('_'.join)\n",
    "create_table_headers(df_input_csv) \n",
    "\n",
    "\n",
    "#print('\\n' + ' column_inserts:  ', column_inserts, '\\n') \n",
    "#print('\\n' + 'column_question_mark:  ', column_question_mark, '\\n') \n",
    "#print('\\n' + 'insert_records_SQL:  ', insert_records_SQL, '\\n')\n",
    "print('\\n' + 'create_table_SQL:  ', create_table_SQL, '\\n')\n",
    "print('\\n' + 'create_real_table_SQL:  ', create_real_table_SQL, '\\n')\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyappointment_df = pd.read_csv(r'C:\\DailyAppointment_A_J_Test\\Main 3.1 to 9.1.xlsx', low_memory = False, header = [0,1])\n",
    "Main 3.1 to 9.1.xlsx\n",
    "\n",
    "Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\n",
    "dailyappointment_df.columns = dailyappointment_df.columns.map('_'.join)\n",
    "dailyappointment_df = dailyappointment_df.fillna(0)\n",
    "dailyappointment_df['Unnamed: 3_Appt Length'] = dailyappointment_df['Unnamed: 3_Appt Length'].astype(int)\n",
    "#...\n",
    "engine = sqlalchemy.create_engine(\n",
    "               \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\",\n",
    "               echo=False)\n",
    "# # df = pd.read_sql_query('SELECT * FROM pbic_1_0.Access',conn)\n",
    "import time \n",
    "start_time1 = time.time()\n",
    "#dailyappointment_df.to_sql('dailyappointment_test', con=engine, if_exists='replace')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time1 = time.time()\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 2.28 to 8.28.xlsx' )\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.2 to 9.2.xlsx' )\n",
    "\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.2 to 8.2.xlsx' )\n",
    "\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.2.xlsx' )\n",
    "\n",
    "Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "Available_Slots_df.rename(columns={'Doctor Name' : 'Doctor_Name','Loc UId' : 'Loc_UId'}, inplace = True)\n",
    " \n",
    "shape = Available_Slots_df.shape\n",
    "print('\\nDataFrame Shape :', shape)\n",
    "print('\\nNumber of rows :', shape[0])\n",
    "print('\\nNumber of columns :', shape[1])\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "file_read_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('Read raw file to Pandas Read Time',f'{file_read_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n",
    "create_table_headers(Available_Slots_df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    \n",
    "    extension = 'xlsx'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "list_all_csv_files('Z:/GPPC_SOURCE_FILES/Oneday_data4_1_22/Available_Slots/')   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "\n",
    "Available_Slots_df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=20000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "\n",
    "\n",
    "# shape = Available_Slots_df.shape\n",
    "# print('\\nDataFrame Shape :', shape)\n",
    "# print('\\nNumber of rows :', shape[0])\n",
    "# print('\\nNumber of columns :', shape[1])\n",
    " \n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "execute_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('SQL Insert Execution Time',f'{execute_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_headers(Available_Slots_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE [pbic_1_0].[Available_Slots](\n",
    "\t[Date] [nvarchar](20) NULL,\n",
    "\t[Day] [nvarchar](50) NULL,\n",
    "\t[Time] [nvarchar](20) NULL,\n",
    "\t[Length] [int] NULL,\n",
    "\t[Dr] [nvarchar](50) NULL,\n",
    "\t[Doctor_Name] [nvarchar](max) NULL,\n",
    "\t[Loc] [nvarchar](50) NULL,\n",
    "\t[Loc_UId] [nvarchar](50) NULL,\n",
    "\t[Type] [nvarchar](50) NULL\n",
    ") ON [PRIMARY] TEXTIMAGE_ON [PRIMARY]\n",
    "GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Available_Slots_df.info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "drop_table_SQL(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[dailyappointment_test]'\n",
    "drop_SQL_table(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Hx', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype = \n",
    "{'datefld': sqlalchemy.DateTime(), \n",
    "'intfld': sqlalchemy.types.INTEGER(),\n",
    "'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "'booleanfld': sqlalchemy.types.Boolean,\n",
    "'bool' : sqlalchemy.types.Boolean,\n",
    "'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "'int64' : sqlalchemy.types.INTEGER(),\n",
    "'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rows(insert_SQL):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the parent directory for all the data subdirectories \n",
    "parent_dir = 'Y:/_Kaleida_Input/' #path to folder that contians the data folders\n",
    "\n",
    "path = parent_dir\n",
    "import_file_type = '\\*.csv'\n",
    "create_table_SQL = ''\n",
    "insert_records_SQL = '' \n",
    "create_schema_SQL = '' \n",
    "column_inserts  = ''\n",
    "column_question_mark  = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_import_files(data_directory_path, import_file_type ):\n",
    "    all_files = glob.glob(data_directory_path + import_file_type)\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "   ...: print('Insert SQL: ', insert_SQL)\n",
    "   ...: insert_row_SQL(insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_headers(input_data_frame):\n",
    "    global column_inserts  \n",
    "    global column_question_mark  \n",
    "    global create_table_SQL \n",
    "    global insert_records_SQL \n",
    "\n",
    "    Table_Name = 'Access'\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes \n",
    "    col_number = 0 \n",
    "    column_inserts = '' \n",
    "    column_creates = '' \n",
    "    column_question_mark = '' \n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "        if len(column_inserts) > 1:\n",
    "            column_inserts = column_inserts + \", \"\n",
    "        if len(column_creates) > 1:\n",
    "            column_creates = column_creates + \", \"     \n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64':  \n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name \n",
    "        \n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type \n",
    "        column_question_mark = column_question_mark + \"?, \"\n",
    "        print(col_number, '  ', column_name) \n",
    "    #print('column_inserts:  ', column_inserts) \n",
    "    #print('column_question_mark:  ', column_question_mark) \n",
    "    print('column_creates:  ', column_creates)  \n",
    "\n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name + '(' + column_inserts + ') VALUES (' + column_question_mark + ');' \n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name + '(' + column_creates + ');' \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = iterate_import_files('Y:\\_Kaleida_Input\\Access','\\\\*.csv')\n",
    "# print(all_files[1])\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, nrows=10)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('C:\\Data\\Behavioral Health'+ '\\*.csv')\n",
    "print(all_files)\n",
    "print(all_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:41:00.444921Z",
     "start_time": "2022-08-30T12:40:48.868620Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove all csv files from dir and unzip folder\n",
    "parent_dir = 'C:/Power BI/' #path to folder\n",
    "path = parent_dir\n",
    "\n",
    "#get csv list\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        os.remove(path+file)    \n",
    "        \n",
    "with ZipFile(path+'PowerBiDownload.zip', 'r') as zipObj:\n",
    "   zipObj.extractall(path)   \n",
    "\n",
    "#remove files\n",
    "#remove files not in list\n",
    "csv_path = r'S:\\Data Team\\Source Data\\python sql\\needed tables.csv'\n",
    "ext = \".csv\"\n",
    "with open(csv_path, 'r') as csvfile:\n",
    "    good_files = []\n",
    "    for n in csv.reader(csvfile):\n",
    "        if len(n) > 0: good_files.append(n[0])\n",
    "    all_files = os.listdir(path)\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(ext) and filename not in good_files:\n",
    "            full_file_path = os.path.join(path, filename)\n",
    "            os.remove(full_file_path)\n",
    "\n",
    "print('Old files removed, new files unzipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:44:43.698446Z",
     "start_time": "2022-08-30T12:41:04.155886Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pull in helper tables, covert to csv and delete old helper tables\n",
    "with open('S:/Data Team/Source Data/python sql/helper tables.csv', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in linereader:\n",
    "        name = row[0]\n",
    "        shutil.copy(name, 'C:\\Power BI\\\\' + os.path.basename(name))\n",
    "                \n",
    "print('All Helper Tables Moved')\n",
    "\n",
    "searchdir = 'C:\\Power BI\\\\'\n",
    "\n",
    "for xls_file in glob.glob(os.path.join(searchdir,\"*.xlsx\")):\n",
    "    data_xls = pd.read_excel(xls_file, index_col = None)\n",
    "    csv_file = os.path.splitext(xls_file)[0]+\".csv\"\n",
    "    data_xls.to_csv(csv_file, encoding = 'utf-8', index = False)\n",
    "    \n",
    "print('All Helper Tables Changed to CSV')\n",
    "\n",
    "df_pipe = pd.read_csv('C:/Power BI/hec daily.txt', delimiter = '|', index_col = None, header = None, on_bad_lines='skip')\n",
    "\n",
    "df_pipe.to_csv('C:/Power BI/hec daily.csv', sep = ',', header = False, index = False)    \n",
    "    \n",
    "print('HeC Daily Converted to CSV')\n",
    "\n",
    "pathtodelete =r\"C:\\Power BI\"\n",
    "filenames_xlsx = glob.glob(pathtodelete + \"/*.xlsx\")\n",
    "for i in filenames_xlsx:\n",
    "    os.remove(i)\n",
    "    \n",
    "filenames_txt = glob.glob(pathtodelete + \"/*.txt\")\n",
    "for k in filenames_txt:\n",
    "    os.remove(k)    \n",
    "    \n",
    "print('Old Helper Tables Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:27:48.939756Z",
     "start_time": "2022-08-30T13:27:35.810299Z"
    }
   },
   "outputs": [],
   "source": [
    "#rename files longer than >=63 char\n",
    "for filename in os.listdir(path):\n",
    "    if len(filename) > 63:\n",
    "        os.rename(path+filename, path+filename[-60:])\n",
    "        print(filename+' renamed to '+filename[-60:])\n",
    "            \n",
    "#get csv list\n",
    "csv_files = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)\n",
    "        \n",
    "data_path = path\n",
    "#create dataframes\n",
    "df = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(data_path+file, low_memory=False, index_col=False)\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        df[file] = pd.read_csv(data_path+file, encoding=\"cp437\", low_memory=False, index_col=False, errors='ignore')\n",
    "    \n",
    "    print('Loading ' + file + ' into dataframe')    \n",
    "print('loading completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:34:56.034850Z",
     "start_time": "2022-08-30T13:28:02.768562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in csv_files:\n",
    "    \n",
    "    dataframe = df[k]\n",
    "    \n",
    "    clean_tbl_name = k.lower().replace(\" \",\"_\").replace(\"-\",\"\").replace(\".\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\\\n",
    "    .replace(\"_csv\", \"\").replace(\"___\", \"_\").replace(\"__\", \"_\")    \n",
    "    \n",
    "    tbl_name = clean_tbl_name\n",
    "    \n",
    "    print(k + ' changing to ' + clean_tbl_name)\n",
    "\n",
    "    #clean column names\n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"-\", \"\").replace(\"#\",\"num\").replace(\"?\", \"\")\\\n",
    "                     .replace(\"=\",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\r\\n\",\"\").replace(\"]\",\"_\")\\\n",
    "                     .replace(\"]\",\"_\").replace(\"[\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\")\\\n",
    "                     .replace(\"%\",\"\").replace(\"#\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"?\",\"\")\\\n",
    "                     .replace(\",\",\"\").replace(\"*\",\"\").replace(\":\",\"\").replace(\"'\",\"\").replace(\"&\",\"\")\\\n",
    "                     .replace(\";\",\"\").replace(\"__\", \"_\").replace(\"/\", \"\")\n",
    "                     for x in dataframe.columns]\n",
    "\n",
    "     #limit column length to 64 and reading right to left\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "\n",
    "     #adding a number if duplicated column name\n",
    "    def uniquify(dataframe):\n",
    "        seen = set()\n",
    "\n",
    "        for item in dataframe:\n",
    "            fudge = 1\n",
    "            newitem = item\n",
    "\n",
    "            while newitem in seen:\n",
    "                fudge += 1\n",
    "                newitem = \"{}_{}\".format(item, fudge)\n",
    "\n",
    "            yield newitem\n",
    "            seen.add(newitem)\n",
    "\n",
    "    dataframe.columns = uniquify(dataframe)\n",
    "\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "    \n",
    "    #db settings and connection\n",
    "    #get password\n",
    "    f=open(\"S:/Data Team Secure/secrets/postgres.txt\",\"r\")\n",
    "    lines=f.readlines()\n",
    "    password=lines[1]\n",
    "    f.close()\n",
    "    \n",
    "   \n",
    "    user=\"Joes_User_Name\"\n",
    "    host = 'Joes_Host_Name'\n",
    "    dbname = 'postgres'\n",
    "        \n",
    "    engine = create_engine('postgresql://'+user+':'+password+'@'+host+'/'+dbname)\n",
    "   \n",
    "    #print('opened database successfully')\n",
    "    \n",
    "    #create table\n",
    "    #dataframe.to_sql(k, engine, schema = None, if_exists='append', index=False, dtype = 'text')\n",
    "    dataframe.to_sql(clean_tbl_name, engine, schema = None, if_exists='append', index=False, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "    \n",
    "    print(clean_tbl_name+' uploaded to database')\n",
    "    \n",
    "print('All uploads complete')\n",
    "\n",
    "## for automation, send email to email list when complete\n",
    "#get email and file list\n",
    "email_list = pd.read_csv('S:/Data Team/Source Data/python sql/email_db_upload.csv')\n",
    "emails = email_list['email']\n",
    "\n",
    "# email loop\n",
    "for i in range(len(emails)):\n",
    "    \n",
    "    email = emails[i]\n",
    "    \n",
    "    # Open the Outlook\n",
    "    outlook = win32.Dispatch('outlook.application')\n",
    "\n",
    "    # Create the email\n",
    "    mail = outlook.CreateItem(0)\n",
    "\n",
    "    # Set the email subject\n",
    "    mail.Subject = 'AUTOMATED EMAIL: Database Updated '+ datetime.now().strftime('%b %#d %Y %H:%M')\n",
    "\n",
    "    # Set the receiver email\n",
    "    mail.To = email\n",
    "\n",
    "    # Write the email content\n",
    "    mail.HTMLBody = r\"\"\"\n",
    "    <p>Hello</p>\n",
    "    <p>The database has had been updated successfully.</p>\n",
    "    <p>Thanks</p>\n",
    "    <p>The Data Team</p>\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the email\n",
    "    mail.Send()\n",
    "    print('Email sent to ' + email)\n",
    "print('All Emails Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:40:48.319174Z",
     "start_time": "2022-08-30T12:40:47.247849Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlalchemy as db\n",
    "# import psycopg2\n",
    "# import os\n",
    "# from sqlalchemy import create_engine\n",
    "# import os, time\n",
    "# import csv\n",
    "# import sqlalchemy\n",
    "# import win32com.client as win32\n",
    "# from datetime import datetime\n",
    "# from datetime import timedelta\n",
    "# from datetime import date\n",
    "# from zipfile import ZipFile\n",
    "# import glob, os\n",
    "# import shutil\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create some Global Variables \n",
    "global column_inserts  \n",
    "global column_question_mark  \n",
    "global create_table_SQL \n",
    "global insert_records_SQL \n",
    "global create_schema_SQL  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIP Install any Python Libraries you dont already have installed \n",
    "#!pip install pyttsx3\n",
    "#!pip install pandas\n",
    "#!pip install pyodbc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

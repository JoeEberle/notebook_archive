{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Precursor Step  - Import Dependent Python Libraries  (If Not Installed) \n",
    "#####  PIP Install any Python Libraries below that you dont already have installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIP Install any Python Libraries you dont already have installed \n",
    "#!pip install pyttsx3\n",
    "#!pip install pandas\n",
    "#!pip install pyodbc\n",
    "#!pip import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL Schema from CSV Files \n",
    "#### Joe Eberle, Alan Calhoun, Helmi (Al)  Seoud\n",
    "##### Refactored ON  : 9/20/2022  ---  Revised ON  : 10/6/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup - Importing Libraries and Initializing Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependent Libraries is not already installed \n",
    "#!pip install pyttsx3\n",
    "\n",
    "# Import the necessary Libraries \n",
    "import glob, os\n",
    "import pandas as pd\n",
    "# import logging \n",
    "from pathlib import Path\n",
    "import pyttsx3\n",
    "import pyodbc \n",
    "import timeit\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sqlalchemy\n",
    "\n",
    "# Establish some import parameters \n",
    "\n",
    "importing_xlsx_files = False \n",
    "importing_csv_files = True \n",
    "#Data_Import_Starting_Directory = 'Y:/_Kaleida_Input/'\n",
    "Data_Import_Starting_Directory = 'C:/Data/'\n",
    "Process_Name = 'Importing CSV data into SQL'\n",
    "\n",
    "step_debugging = True\n",
    "detail_debugging = True\n",
    "detail_Talking = False # only talk on major steps \n",
    "Process_Step_Name = ''  \n",
    "Reading_Intro = True\n",
    "Reading_Credits =  True\n",
    "Reading_Steps = True \n",
    "Reading_Terms = True \n",
    "printing_output = True\n",
    "Talking_Code = True\n",
    "Talking_Voice_Male_Gender = True        # Set to False for Female Voice \n",
    "Code_Logging  = True \n",
    "event_log_row = 0 \n",
    "\n",
    "# Create some Global Variables for SQL Constructs \n",
    "Table_Name_Extension_Daily = '_DI'\n",
    "Table_Name_Extension_Historical = '_HX'\n",
    "Table_Name_Extension_Administrative = '_AD'\n",
    "Table_Name_Prefix = '[pbic_1_0].'\n",
    "\n",
    "# Create some Global Variables for SQL Connection\n",
    "server = 'Kalpwvsqlgppc01' \n",
    "database  = 'GPPC_DEV' \n",
    "username ='GPPC'\n",
    "pwd = 'Elephant-Trunk-06'\n",
    "sql_connector = 'DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd\n",
    "# log_filename='data_importing_log.log'\n",
    "\n",
    "# Configure the Logging to the DEBUG Level \n",
    "# logging.basicConfig(level=logging.DEBUG, filename=log_filename, format= '%(asctime)s %(clientip)-15s %(user)-8s %(message)s')\n",
    "Text_to_Speech = pyttsx3.init()\n",
    "\n",
    "## Dictionary For character_replacements List \n",
    "character_replacements = { \" \":\"_\",\"#\":\"Number\",\"%\":\"Percentage\" \\\n",
    "                         ,'_Unnamed':'','_Level':'',\"$\":\"Dollar\",'_1':'' \\\n",
    "                         ,'_2':'','_3':'','_4':'','_5':''  \\\n",
    "                         ,'_6':'','_7':'','_8':'','_9':''  \\\n",
    "                         ,'_0':'',':7':'',':8':'',':':'' }\n",
    "## Dictionary For replacing data types in databases \n",
    "data_type_replacements = { \"object\":\"varchar\",\"float64\":\"float\",\"int64\":\"int\",\"%\":\"Percentage\" \\\n",
    "                         ,'_Unnamed':'','datetime64':'timestamp',\"timedelta64[ns]\":\"varchar\"}    \n",
    "\n",
    "## add the glaobal data frames for event loggging and Schema Creation \n",
    "df_event_log = pd.DataFrame(columns = ('Event_ID','Process_Name','Event_Name','Event_Date','Event_Time','Task_Start_Time','Task_End_Time','TASk_Duration','Comments'))\n",
    "df_import_directories = pd.DataFrame(columns = ('Root_Directory','Sub_Directory'))\n",
    "df_import_files = pd.DataFrame(columns = ('Root_Directory','Sub_Directory','Table_Name','Import_File_Name'))\n",
    "import_directory_file_Number  = 0\n",
    "import_file_Number  = 0 \n",
    "event_log_row = 0\n",
    "sub_directory_count = 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup -    Establishing DataFrames & Establishing Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global Talking_Code\n",
    "global import_directory_file_Number \n",
    "global event_log_row \n",
    "global printing_output\n",
    "\n",
    "\n",
    "def set_up_python_infrastructure():\n",
    "    initialize_replacement_Dictionaries()    # Set up control libraries for syntactic Consistency \n",
    "    Initialize_Text_to_Speach()              # Intitialize Text to Speech Engine \n",
    "    df_e_log = create_event_log_dataframe()  # Set up the Event Logging \n",
    "    \n",
    "# Say Whatever the user wants \n",
    "def say(speech):\n",
    "    if Talking_Code:\n",
    "        Text_to_Speech.say(speech)\n",
    "        Text_to_Speech.runAndWait()    \n",
    "    \n",
    "# Intitialize Text to Speech Engine \n",
    "def Initialize_Text_to_Speach():\n",
    "    Text_to_Speech = pyttsx3.init()\n",
    "    Text_to_Speech.setProperty('Rate',187)\n",
    "    voices = Text_to_Speech.getProperty('voices')\n",
    "    if Talking_Voice_Male_Gender:\n",
    "        Text_to_Speech.setProperty('voice', voices[0].id)    # Default Male voice registered as 'Dave'\n",
    "    else: \n",
    "        Text_to_Speech.setProperty('voice', voices[1].id)    # Alternate Female voice registered as 'Tina'\n",
    "    speech = 'The text to speech engine is initialized using pythons pyttsx3 engine'\n",
    "    Text_to_Speech.say(speech)\n",
    "    Text_to_Speech.runAndWait()    \n",
    "    \n",
    "    \n",
    "# Say Whatever the user wants \n",
    "def say(speech):\n",
    "    Text_to_Speech.say(speech)\n",
    "    Text_to_Speech.runAndWait()       \n",
    "    \n",
    "# Create dataframe to house Directories \n",
    "def create_directory_dataframe():\n",
    "    df_import_directories = pd.DataFrame(columns = ('Root_Directory','Sub_Directory'))\n",
    "    return df_import_directories  \n",
    "\n",
    "def create_import_files_dataframe():\n",
    "    df_import_files = pd.DataFrame(columns = ('Root_Directory','Sub_Directory','Table_Name','File_Name'))\n",
    "    return df_import_files  \n",
    "\n",
    "def add_import_directory(Root_Directory,Sub_Directory):\n",
    "    global import_directory_file_Number \n",
    "    import_directory_file_Number += 1  \n",
    "    df_import_directories.loc[import_directory_file_Number] = [Root_Directory,Sub_Directory]\n",
    "    return import_directory_file_Number \n",
    "\n",
    "def add_import_File(Root_Directory,Sub_Directory,Table_Name,File_Name):\n",
    "    global import_file_Number \n",
    "    import_file_Number += 1  \n",
    "    df_import_files.loc[import_file_Number] = [Root_Directory,Sub_Directory,Table_Name,File_Name]\n",
    "    return import_directory_file_Number  \n",
    "\n",
    "# Create dataframe to house Directories \n",
    "def create_event_log_dataframe():\n",
    "    df_event_log = pd.DataFrame(columns = ('Event_ID','Process_Name','Event_Name','Event_Date','Event_Time','Task_Start_Time','Task_End_Time','TASk_Duration','Comments'))\n",
    "    return df_event_log\n",
    "\n",
    "\n",
    "df_event_log = create_event_log_dataframe()\n",
    "\n",
    "# Create dataframe to database schema \n",
    "def create_database_schema_dataframe():\n",
    "    df_schema = pd.DataFrame(columns = ('Database_Name','Table_Name','Column_Number','Column_Name','Column_Data_Type','Column_Sample_Data','Column_Description'))\n",
    "    return df_schema\n",
    "\n",
    "# Create dataframe to house Directories \n",
    "def add_log_event(Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments ):\n",
    "    global event_log_row  \n",
    "    event_log_row += 1  \n",
    "    df_event_log.loc[event_log_row] = [event_log_row,Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments]\n",
    "\n",
    "    \n",
    "# Add Log Events \n",
    "def add_log_event_timer(Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments ):\n",
    "    global event_log_row  \n",
    "    event_log_row += 1  \n",
    "    Event_Date = date.today()\n",
    "    Event_Time = time.time() \n",
    "    df_event_log.loc[event_log_row] = [event_log_row,Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments]\n",
    "    \n",
    "# Reset the Event timer start time \n",
    "def reset_event_timer(Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments ):\n",
    "    global event_log_row  \n",
    "    event_log_row += 1  \n",
    "    Event_Date = date.today()\n",
    "    Event_Time = time.time() \n",
    "    Task_Start_Time    = time.time()  \n",
    "    df_event_log.loc[event_log_row] = [event_log_row,Process_Name,Event_Name,Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , Comments]\n",
    "\n",
    "    # Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        out('CSV File #{} filename: {}  '.format(str(csv_file_count),file))\n",
    "        \n",
    "    # Get a list of all the Subfiles to iterate through \n",
    "def register_all_csv_files_for_import(path, table_name):\n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        out('CSV File #{} filename: {}  '.format(str(csv_file_count),file)) \n",
    "        file_Number = add_import_File(path,path,table_name,file)\n",
    "        \n",
    "def create_import_files_dataframe():\n",
    "    df_import_files = pd.DataFrame(columns = ('Root_Directory','Sub_Directory','Table_Name','File_Name'))\n",
    "    return df_import_files  \n",
    "\n",
    "def add_import_directory(Root_Directory,Sub_Directory):\n",
    "    global import_directory_file_Number \n",
    "    import_directory_file_Number += 1  \n",
    "    df_import_directories.loc[import_directory_file_Number] = [Root_Directory,Sub_Directory]\n",
    "    return import_directory_file_Number \n",
    "\n",
    "def add_import_File(Root_Directory,Sub_Directory,Table_Name,File_Name):\n",
    "    global import_file_Number \n",
    "    import_file_Number += 1  \n",
    "    df_import_files.loc[import_file_Number] = [Root_Directory,Sub_Directory,Table_Name,File_Name]\n",
    "    return import_directory_file_Number          \n",
    "        \n",
    "    \n",
    "def infer_table_name_from_path(path):\n",
    "    table_name = path.replace(Data_Import_Starting_Directory,\"\").replace(' ','_').replace('/','').replace('\\\\','')\n",
    "    return table_name \n",
    "        \n",
    "# Introduction - Overview of CSV to SQL Import Process Steps \n",
    "def read_credits(): \n",
    "    Dialog = 'This Jupiter Notebook Was  : '\n",
    "    Dialog = Dialog + 'Developed in Collaboration by Joe Eberle, Alan Calhoun, Helmi (Al) Seoud  '\n",
    "    Dialog = Dialog + 'Developed in Python starting on 9/20/2022 '\n",
    "    Dialog = Dialog + 'This package is free AND Open Source and the code is openly available for general Use. '    \n",
    "    say(Dialog)         \n",
    "    \n",
    "# Introduction - Overview of CSV to SQL Import Process Steps \n",
    "def read_terms(): \n",
    "    Dialog = 'The terminology for this process is : '\n",
    "    Dialog = Dialog + 'Python. Python is a general-purpose programming language that is widely used for data science.  '\n",
    "    Dialog = Dialog + 'Structured Query Language (SQL) is one of the worlds most widely used programming languages for manipulating and querying data. '\n",
    "    Dialog = Dialog + 'CSV. A Comma-Separated Values (CSV)  file is a text file in which information is separated by commas. '\n",
    "    Dialog = Dialog + 'PANDAS. Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.  '\n",
    "    Dialog = Dialog + 'OS PACKAGE - The OS python library provides a portable way of using operating system dependent functionality to allow your python code to run on all platforms '\n",
    "    say(Dialog)  \n",
    "    \n",
    "# Process Steps - Overview of CSV to SQL Import Process Steps \n",
    "def read_process_steps():\n",
    "    Dialog = 'The data flow for this process is : '\n",
    "    Dialog = Dialog + 'Precursor Step 1: The clinician or administrator enters the patients data into the Electronic Medical Record (EMR). '\n",
    "    Dialog = Dialog + 'Precursor Step 2: At the end of the day the EMR data is exported into Comma Seperated Values (CSV) files and shared via SFTP. '\n",
    "    Dialog = Dialog + 'Step 1: Establish The Root Directory. '\n",
    "    Dialog = Dialog + 'Step 2: Walk the directory structure discovering data to discover all data directories  '\n",
    "    Dialog = Dialog + 'Step 3: Read the CSV data from each directory into python a PANDAS Dataframe. '\n",
    "    Dialog = Dialog + 'Step 4: Clean the data and make it consistent in the PANDAS Dataframe. ' \n",
    "    Dialog = Dialog + 'Step 5: Check the consistency of the data and perform change control if there are differences. ' \n",
    "    Dialog = Dialog + 'Step 6: Convert the pandas dataframes into SQL table Create Statements  '\n",
    "    Dialog = Dialog + 'Step 7: Creates the SQL tables in the target Database   '\n",
    "    Dialog = Dialog + 'Step 8: Insert the the PANDAS Rows into SQL using the to_SQL Method.  '\n",
    "    Dialog = Dialog + 'Step 9: Add event logging to capture the performance of the entire process.  '\n",
    "    Dialog = Dialog + 'Step 10: Document the SCHEMA into an easy to use Excel Spreadsheet.  '\n",
    "    Dialog = Dialog + 'Step 11: Check the total number of records imported via SQL to the total raw record count to make sure no data is Left Behind.    '\n",
    "    say(Dialog)    \n",
    "    \n",
    "    \n",
    "# Introduction - Overview of NoteBooks  \n",
    "def read_introduction():\n",
    "    Dialog = 'This jupiter notebook will import all of the CSV files under a specific root directory into a database. '\n",
    "    Dialog = Dialog +  'This python code will take the CSV files exported froms an Electronic Medical Record platform. '\n",
    "    Dialog = Dialog + 'and import them into a faster database such as PostgreSQL or SQL Server or SNOW Flake. '\n",
    "    Dialog = Dialog + 'the data is then available for anaylsis using query tools or ready for visualizations in Power BI or Tableau. '\n",
    "    say(Dialog)  \n",
    "    \n",
    "    \n",
    "def column_create_SQL (import_df):\n",
    "    column_name_List = [x.title() for x in import_df.columns] # Create a List of Columns \n",
    "    column_Str =  (', '.join(column_name_List)) # Convert List into one String with commas \n",
    "    out('Columns =',column_Str)  \n",
    "    return column_Str            \n",
    "    \n",
    "    \n",
    "def out(dialog):\n",
    "    global detail_Talking\n",
    "    dialog = dialog.lower() \n",
    "    if printing_output: \n",
    "        print(dialog) \n",
    "    if Talking_Code and detail_Talking == True:\n",
    "        say(dialog)   \n",
    "    if Talking_Code and (detail_Talking == False) and (dialog.find('step') >= 0):\n",
    "        say(dialog)     \n",
    "\n",
    "        \n",
    "def list_all_xlsx_files(path):\n",
    "    extension = 'xlsx'\n",
    "    os.chdir(path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        out('File #{}   is {} '.format(csv_file_count,file))     \\\n",
    "        \n",
    "        \n",
    "def explain_the_project():\n",
    "    if Reading_Intro:\n",
    "        read_introduction()\n",
    "    if Reading_Credits:    \n",
    "        read_credits() \n",
    "    if Reading_Steps:\n",
    "        read_process_steps()\n",
    "    if Reading_Terms:\n",
    "        read_terms()        \n",
    "        \n",
    "        \n",
    "        \n",
    "def convert_data_types(input_df):\n",
    "    column_datatype_str = str(input_df.dtypes)\n",
    "    column_datatype_str =  column_datatype_str.replace('dtype: object','').replace('object','varchar[255], ').replace('datetime64[ns]','timestamp, ').replace('float64','float, ')\n",
    "    out('create column SQL string: {} \\n'.format(column_datatype_str))        \n",
    "        \n",
    "def read_and_clean_file(data_folder, filename):\n",
    "\n",
    "    print('Import File =', filename)                 \n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    out('Reading Dataframe Columns before cleanups:{}'.format(df_input_csv.columns))\n",
    "    df_input_csv.columns = df_input_csv.columns.map('^'.join)\n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "    return df_input_csv    \n",
    " \n",
    "        \n",
    "    \n",
    "def set_up_python_infrastructure():\n",
    "    initialize_replacement_Dictionaries()    # Set up control libraries for syntactic Consistency \n",
    "    Initialize_Text_to_Speach()              # Intitialize Text to Speech Engine \n",
    "    df_event_log = create_event_log_dataframe()  # Set up the Event Logging to housae the events of this process \n",
    "    create_database_schema_dataframe()       # Set up the Database Schema dataframe to house the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup -    Database Connectivity & SQL Generation & SQL Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SQL  Dynamically based upon the Connection string  \n",
    "def execute_SQL(execute_SQL_command):\n",
    "    global sql_connector\n",
    "    print('Execute SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(execute_SQL_command)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    out('Executing SQL - After SQL Execute - Command: {} '.format(execute_SQL_command))    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()\n",
    "    \n",
    "# The following code uses a SQL server template for Droping tables and replaces the table name \n",
    "# spo that it will create the SQL code will drop ANY Table name that is passed to it \n",
    "def Create_Drop_Table_SQL(table_name):  \n",
    "    # Example ---- DROP TABLE [pbic_1_0].[Access_DI]\n",
    "    Drop_Table_SQL  = 'DROP TABLE [pbic_1_0].[{}]'.format(table_name)\n",
    "    return Drop_Table_SQL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talking Code Setting:  True\n",
      "Talking Code Detail Setting:  False\n",
      "Talking_Code: Text to Voice set to ON. Voice set  to  Default Male Voice. \n",
      "Talking  Details is set to True say only major steps  ...  this will fast and provide high level outline \n"
     ]
    }
   ],
   "source": [
    "print('Talking Code Setting: ',Talking_Code)\n",
    "print('Talking Code Detail Setting: ',detail_Talking)\n",
    "if Talking_Code:\n",
    "    if Talking_Voice_Male_Gender: \n",
    "        out('Talking_Code: Text to Voice set to ON. Voice set  to  Default Male Voice. ')\n",
    "    else: \n",
    "        out('Talking_Code: Text to Voice set to ON. Voice set  to  Alternate Female Voice. ')\n",
    "    if  detail_Talking:   \n",
    "        out('Talking  Details is set to True say all detailed outputs...  this will be slow and boring ') \n",
    "    else: \n",
    "        out('Talking  Details is set to True say only major steps  ...  this will fast and provide high level outline ')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Process Reset - Re initialize the Data Frames and all Global Counters to Zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "global import_directory_file_Number \n",
    "global import_file_Number  \n",
    "global event_log_row \n",
    "global sub_directory_count  \n",
    "\n",
    "# recreate the dataframes \n",
    "create_directory_dataframe() \n",
    "create_import_files_dataframe() \n",
    "create_database_schema_dataframe() \n",
    "\n",
    "# reset all the global Counters  \n",
    "import_directory_file_Number  = 0\n",
    "import_file_Number  = 0 \n",
    "event_log_row = 0\n",
    "sub_directory_count = 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Precursor Step  - Explain the Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_the_project()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Establish the root Directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 - establish the root directory\n",
      "the root directory to walk is : c:/data/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>Process_Name</th>\n",
       "      <th>Event_Name</th>\n",
       "      <th>Event_Date</th>\n",
       "      <th>Event_Time</th>\n",
       "      <th>Task_Start_Time</th>\n",
       "      <th>Task_End_Time</th>\n",
       "      <th>TASk_Duration</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>step 1 - Establish the root Directory: C:/Data/</td>\n",
       "      <td>2022-10-13 00:25:28.713598</td>\n",
       "      <td>2022-10-13 00:25:28.713598</td>\n",
       "      <td>1.665635e+09</td>\n",
       "      <td>1.665635e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>Step 1 - Establish the root Directory: C:/Data/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Importing CSV data into SQL</td>\n",
       "      <td>step 1 - Establish the root Directory: C:/Data/</td>\n",
       "      <td>2022-10-13 00:25:39.754033</td>\n",
       "      <td>2022-10-13 00:25:39.754033</td>\n",
       "      <td>1.665635e+09</td>\n",
       "      <td>1.665635e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>Step 1 - Establish the root Directory: C:/Data/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Event_ID                 Process_Name  \\\n",
       "5        5  Importing CSV data into SQL   \n",
       "6        6  Importing CSV data into SQL   \n",
       "\n",
       "                                        Event_Name                 Event_Date  \\\n",
       "5  step 1 - Establish the root Directory: C:/Data/ 2022-10-13 00:25:28.713598   \n",
       "6  step 1 - Establish the root Directory: C:/Data/ 2022-10-13 00:25:39.754033   \n",
       "\n",
       "                  Event_Time  Task_Start_Time  Task_End_Time TASk_Duration  \\\n",
       "5 2022-10-13 00:25:28.713598     1.665635e+09   1.665635e+09             0   \n",
       "6 2022-10-13 00:25:39.754033     1.665635e+09   1.665635e+09             0   \n",
       "\n",
       "                                          Comments  \n",
       "5  Step 1 - Establish the root Directory: C:/Data/  \n",
       "6  Step 1 - Establish the root Directory: C:/Data/  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_log_event(Process_Name,'step 1 - Establish the root Directory: {}'.format(Data_Import_Starting_Directory),datetime.now(),datetime.now(),time.time(),time.time(), 0 ,'Step 1 - Establish the root Directory: {}'.format(Data_Import_Starting_Directory))\n",
    "Process_Step_Name = 'Step 1 - Establish the root Directory' \n",
    "out('Step 1 - Establish the root Directory')\n",
    "out('The root directory to walk is : {}'.format(Data_Import_Starting_Directory))  \n",
    "df_event_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Walk the directory structure discovering data to discover all data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2 - walk the directory structure  to discover all data directories\n",
      "registering directory # 0 c:/data/  \n",
      "registering directory # 1 c:/data/behavioral health  \n",
      "registering directory # 2 c:/data/brfss_cdc  \n",
      "registering directory # 3 c:/data/chronic kidney disease  \n",
      "registering directory # 4 c:/data/chronic_disease_indicators  \n",
      "registering directory # 5 c:/data/data_science_data  \n",
      "registering directory # 6 c:/data/data_science_data\\projects  \n",
      "registering directory # 7 c:/data/data_science_data\\projects\\aihs  \n",
      "registering directory # 8 c:/data/data_science_data\\projects\\aihs\\architecture  \n",
      "registering directory # 9 c:/data/data_science_data\\projects\\aihs\\chatbot initial conversation - botsociety_files  \n",
      "registering directory # 10 c:/data/data_science_data\\test_data  \n",
      "registering directory # 11 c:/data/data_science_data\\test_data\\india health stats  \n",
      "registering directory # 12 c:/data/data_science_data\\test_data\\medicare  \n",
      "registering directory # 13 c:/data/data_science_data\\test_data\\mental_health_faq_for_chatbot  \n",
      "registering directory # 14 c:/data/data_science_data\\test_data\\new_kaggle_data  \n",
      "registering directory # 15 c:/data/data_science_data\\test_data\\nhanes  \n",
      "registering directory # 16 c:/data/diabetes  \n",
      "registering directory # 17 c:/data/heart  \n",
      "step 2 done - listing all registered data directories: \n"
     ]
    }
   ],
   "source": [
    "global sub_directory_count\n",
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory):\n",
    "    global sub_directory_count\n",
    "    Process_Step_Name = 'Step 2 - Walk the directory structure discovering data to discover all data directories' \n",
    "    df_import_directories = create_directory_dataframe() \n",
    "    directory_entry = 0 \n",
    " \n",
    "    for root, subdirectories, files in os.walk(root_directory):\n",
    "        # hard coded - remove this later!!!!!! \n",
    "        # old data and Excel data should NOT BE Included under root \n",
    "        if (root.find('old') == -1) and (root.find('excel') == -1):\n",
    "            out('Registering Directory # {} {}  '.format(directory_entry,root ))            \n",
    "            directory_entry += 1 \n",
    "            sub_directory_count += 1     \n",
    "            num = add_import_directory(root, root)        \n",
    "         \n",
    "    return df_import_directories\n",
    "\n",
    "out('Step 2 - Walk the directory structure  to discover all data directories')\n",
    "add_log_event(Process_Name,'Step 2 - Walk the directory structure  to discover all data directories',datetime.now(),datetime.now(),time.time(),time.time(), 0 ,'Step 2 - Walk the directory structure  to discover all data directories'.format(Data_Import_Starting_Directory))\n",
    "walk_sub_directories(Data_Import_Starting_Directory)  \n",
    "out('Step 2 Done - Listing all Registered data Directories: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registering directories in excel file:c:/data/import_directory_registry.xlsx \n"
     ]
    }
   ],
   "source": [
    "# Persist the directories in an excel file Registry \n",
    "Excel_file_Name = Data_Import_Starting_Directory + 'Import_Directory_Registry.xlsx'\n",
    "out('Registering Directories in excel File:{} '.format(Excel_file_Name))\n",
    "df_import_directories.to_excel(Excel_file_Name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_import_directories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Table_SQL  = Create_Drop_Table_SQL('Access_DI')\n",
    "# SQL_Result = execute_SQL(Drop_Table_SQL)\n",
    "# out('SQL Execute Result: {} '.format(SQL_Result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Root_Directory</th>\n",
       "      <th>Sub_Directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Data/</td>\n",
       "      <td>C:/Data/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Data/Behavioral Health</td>\n",
       "      <td>C:/Data/Behavioral Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Data/brfss_cdc</td>\n",
       "      <td>C:/Data/brfss_cdc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Data/Chronic Kidney Disease</td>\n",
       "      <td>C:/Data/Chronic Kidney Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:/Data/Chronic_Disease_Indicators</td>\n",
       "      <td>C:/Data/Chronic_Disease_Indicators</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:/Data/Data_Science_Data</td>\n",
       "      <td>C:/Data/Data_Science_Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS\\Archit...</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS\\Archit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS\\Chatbo...</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Projects\\AIHS\\Chatbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\India Heal...</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\India Heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\Medicare</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\Medicare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\Mental_Hea...</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\Mental_Hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\New_Kaggle...</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\New_Kaggle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\nhanes</td>\n",
       "      <td>C:/Data/Data_Science_Data\\Test_Data\\nhanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C:/Data/Diabetes</td>\n",
       "      <td>C:/Data/Diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C:/Data/Heart</td>\n",
       "      <td>C:/Data/Heart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Root_Directory  \\\n",
       "1                                            C:/Data/   \n",
       "2                           C:/Data/Behavioral Health   \n",
       "3                                   C:/Data/brfss_cdc   \n",
       "4                      C:/Data/Chronic Kidney Disease   \n",
       "5                  C:/Data/Chronic_Disease_Indicators   \n",
       "6                           C:/Data/Data_Science_Data   \n",
       "7                  C:/Data/Data_Science_Data\\Projects   \n",
       "8             C:/Data/Data_Science_Data\\Projects\\AIHS   \n",
       "9   C:/Data/Data_Science_Data\\Projects\\AIHS\\Archit...   \n",
       "10  C:/Data/Data_Science_Data\\Projects\\AIHS\\Chatbo...   \n",
       "11                C:/Data/Data_Science_Data\\Test_Data   \n",
       "12  C:/Data/Data_Science_Data\\Test_Data\\India Heal...   \n",
       "13       C:/Data/Data_Science_Data\\Test_Data\\Medicare   \n",
       "14  C:/Data/Data_Science_Data\\Test_Data\\Mental_Hea...   \n",
       "15  C:/Data/Data_Science_Data\\Test_Data\\New_Kaggle...   \n",
       "16         C:/Data/Data_Science_Data\\Test_Data\\nhanes   \n",
       "17                                   C:/Data/Diabetes   \n",
       "18                                      C:/Data/Heart   \n",
       "\n",
       "                                        Sub_Directory  \n",
       "1                                            C:/Data/  \n",
       "2                           C:/Data/Behavioral Health  \n",
       "3                                   C:/Data/brfss_cdc  \n",
       "4                      C:/Data/Chronic Kidney Disease  \n",
       "5                  C:/Data/Chronic_Disease_Indicators  \n",
       "6                           C:/Data/Data_Science_Data  \n",
       "7                  C:/Data/Data_Science_Data\\Projects  \n",
       "8             C:/Data/Data_Science_Data\\Projects\\AIHS  \n",
       "9   C:/Data/Data_Science_Data\\Projects\\AIHS\\Archit...  \n",
       "10  C:/Data/Data_Science_Data\\Projects\\AIHS\\Chatbo...  \n",
       "11                C:/Data/Data_Science_Data\\Test_Data  \n",
       "12  C:/Data/Data_Science_Data\\Test_Data\\India Heal...  \n",
       "13       C:/Data/Data_Science_Data\\Test_Data\\Medicare  \n",
       "14  C:/Data/Data_Science_Data\\Test_Data\\Mental_Hea...  \n",
       "15  C:/Data/Data_Science_Data\\Test_Data\\New_Kaggle...  \n",
       "16         C:/Data/Data_Science_Data\\Test_Data\\nhanes  \n",
       "17                                   C:/Data/Diabetes  \n",
       "18                                      C:/Data/Heart  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_import_directories.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Discover and Register all CSV files to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3 - discover and register all csv files to import \n",
      "directory #1 to find import files : c:/data/  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/\n",
      "directory #2 to find import files : c:/data/behavioral health  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Behavioral Health\n",
      "csv file #1 filename: 2011.csv  \n",
      "csv file #2 filename: 2012.csv  \n",
      "csv file #3 filename: 2013.csv  \n",
      "csv file #4 filename: 2014.csv  \n",
      "csv file #5 filename: 2015.csv  \n",
      "csv file #6 filename: behavioral_risk_factor_surveillance_system__brfss__historical_questions.csv  \n",
      "csv file #7 filename: behavioral_risk_factor_surveillance_system__brfss__prevalence_data__2011_to_present_.csv  \n",
      "directory #3 to find import files : c:/data/brfss_cdc  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/brfss_cdc\n",
      "csv file #1 filename: 2011.csv  \n",
      "csv file #2 filename: 2012.csv  \n",
      "csv file #3 filename: 2013.csv  \n",
      "csv file #4 filename: 2014.csv  \n",
      "csv file #5 filename: 2015.csv  \n",
      "csv file #6 filename: behavioral_risk_factor_surveillance_system__brfss__age-adjusted_prevalence_data__2011_to_present_.csv  \n",
      "csv file #7 filename: brfss_2015.csv  \n",
      "directory #4 to find import files : c:/data/chronic kidney disease  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Chronic Kidney Disease\n",
      "csv file #1 filename: kidney_disease.csv  \n",
      "directory #5 to find import files : c:/data/chronic_disease_indicators  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Chronic_Disease_Indicators\n",
      "csv file #1 filename: u.s._chronic_disease_indicators.csv  \n",
      "directory #6 to find import files : c:/data/data_science_data  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\n",
      "directory #7 to find import files : c:/data/data_science_data\\projects  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Projects\n",
      "directory #8 to find import files : c:/data/data_science_data\\projects\\aihs  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Projects\\AIHS\n",
      "directory #9 to find import files : c:/data/data_science_data\\projects\\aihs\\architecture  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Projects\\AIHS\\Architecture\n",
      "directory #10 to find import files : c:/data/data_science_data\\projects\\aihs\\chatbot initial conversation - botsociety_files  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Projects\\AIHS\\Chatbot Initial Conversation - Botsociety_files\n",
      "directory #11 to find import files : c:/data/data_science_data\\test_data  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\n",
      "directory #12 to find import files : c:/data/data_science_data\\test_data\\india health stats  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\\India Health Stats\n",
      "csv file #1 filename: key_indicator_districtwise.csv  \n",
      "csv file #2 filename: key_indicator_statewise.csv  \n",
      "directory #13 to find import files : c:/data/data_science_data\\test_data\\medicare  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\\Medicare\n",
      "csv file #1 filename: inpatient_pat.csv  \n",
      "csv file #2 filename: inpatient_provdr.csv  \n",
      "csv file #3 filename: outpatient_pat.csv  \n",
      "csv file #4 filename: outpatient_provdr.csv  \n",
      "csv file #5 filename: patient_history_samp.csv  \n",
      "csv file #6 filename: review_patient_history_samp.csv  \n",
      "csv file #7 filename: rreview_transaction_coo.csv  \n",
      "csv file #8 filename: transaction_coo.csv  \n",
      "directory #14 to find import files : c:/data/data_science_data\\test_data\\mental_health_faq_for_chatbot  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\\Mental_Health_FAQ_for_Chatbot\n",
      "csv file #1 filename: mental_health_faq.csv  \n",
      "directory #15 to find import files : c:/data/data_science_data\\test_data\\new_kaggle_data  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\\New_Kaggle_Data\n",
      "csv file #1 filename: lowbirthweight.csv  \n",
      "csv file #2 filename: meonatl mortality unigme-2021.csv  \n",
      "directory #16 to find import files : c:/data/data_science_data\\test_data\\nhanes  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Data_Science_Data\\Test_Data\\nhanes\n",
      "directory #17 to find import files : c:/data/diabetes  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Diabetes\n",
      "csv file #1 filename: dg.csv  \n",
      "csv file #2 filename: diabetes.csv  \n",
      "csv file #3 filename: diabetes_pima_indians.csv  \n",
      "directory #18 to find import files : c:/data/heart  \n",
      "   \n",
      "CSV Files to Import from Directory: C:/Data/Heart\n",
      "csv file #1 filename: heart.csv  \n"
     ]
    }
   ],
   "source": [
    "def iterate_directories_to_import_files(Import_Directory_Data_Frame):\n",
    "    out('Step 3 - Discover and Register all CSV files to import ')\n",
    "    Process_Step_Name = 'Step 3 - Discover and Register all CSV files to import'  \n",
    "    list_of_Directories = df_import_directories['Root_Directory']\n",
    "    dir_count = 0\n",
    "    for dir in list_of_Directories:\n",
    "        dir_count += 1\n",
    "        out('Directory #{} to find import files : {}  \\n   '.format(dir_count,dir))\n",
    "        SQL_Table_Name = infer_table_name_from_path(dir)\n",
    "        register_all_csv_files_for_import(dir, SQL_Table_Name )\n",
    "\n",
    "iterate_directories_to_import_files(df_import_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registering directories in excel file:c:/data/discovered_csv_files_to_import.xlsx \n"
     ]
    }
   ],
   "source": [
    "# Persist the csv files in an excel import file Registry \n",
    "Excel_file_Name = Data_Import_Starting_Directory + 'Discovered_CSV_files_to_import.xlsx'\n",
    "out('Registering Directories in excel File:{} '.format(Excel_file_Name))\n",
    "df_import_files.to_excel(Excel_file_Name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_import_files.loc[1  , ['Root_Directory','Import_file_Name']]\n",
    "df_import_files.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1934, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_import_files.loc[1  , ['Root_Directory','Import_file_Name']]\n",
    "df_import_files.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_raw_file(raw_csv_file_to_import , table_name , Head_Rows):\n",
    "    out('About to Read File:{}'.format(raw_csv_file_to_import))   \n",
    "    if Head_Rows == 2:\n",
    "        df_import_file = pd.read_csv(raw_csv_file_to_import ,header=[0,1] )\n",
    "        df_import_file.columns = df_import_file.columns.map('^'.join)\n",
    "        \n",
    "        \n",
    "        df_import_file.columns = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('_level_','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','').replace('_Level','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace('0_','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_import_file.columns]  \n",
    "        \n",
    "    else: \n",
    "        df_import_file = pd.read_csv(raw_csv_file_to_import   )   \n",
    "        df_import_file.columns = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_import_file.columns]          \n",
    "    \n",
    "    out('Read in raw csv file:{} rows:{} columns:{}'.format(raw_csv_file_to_import,df_import_file.shape[0], df_import_file.shape[1]))\n",
    "    \n",
    "    return df_import_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global detail_debugging\n",
    "\n",
    "for i, row in df_import_files.iterrows():\n",
    "#    print(' Iter = {}'.format(i))\n",
    "    Index = i \n",
    "    Root_Dir   =   row[0]      \n",
    "    Sub_Dir    =   row[1]    \n",
    "    Table_Name =   row[2]\n",
    "    Import_CSV =   row[3]  \n",
    "    #print('Detail Debugging:', detail_debugging ) \n",
    "    \n",
    "    if detail_debugging:    \n",
    "        out('Iter:{}, Root Dir:{}, Sub_Dir:{}, Table:{}, Import_CSV:{} '.format(i,Root_Dir,Sub_Dir,Table_Name,Import_CSV))\n",
    "    if Table_Name == 'Access':\n",
    "        Header_Row_Count = 2\n",
    "    else:         \n",
    "        Header_Row_Count = 1\n",
    "    \n",
    "    if i == 1:\n",
    "        file_to_import = Root_Dir + '/' + Import_CSV\n",
    "        if detail_debugging:\n",
    "            out(' File to Import: {} '.format(file_to_import))\n",
    "        df_import_raw = inspect_raw_file(file_to_import, Table_Name, Header_Row_Count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_name_clean_up(df_to_clean):\n",
    "    df_to_clean.rename(columns={'Name_':'Name','Appt_Date':'Appointment_Date','Appt_Type':'Appointment_Type' \\\n",
    "                                ,'EbillEsuperbillNumber':'Ebill_Esuper_Bill_Number' \\\n",
    "                                ,'Referral_FromTodo_Selection':'Referral_From_To_Do_Selection' \\\n",
    "                                ,'Wellnow_LocationTodo_Selection':'Wellnow_Location_To_Do_Selection' \\\n",
    "                                ,'Access_Vip':'Access_VIP' \\\n",
    "                                ,'Vip':'VIP' \\\n",
    "                                ,'Todo_DateCreate_Date':'To_Do_Date_Create_Date' \\\n",
    "                               }, inplace = True)\n",
    "    return df_to_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_import_raw = column_name_clean_up(df_import_raw)\n",
    "df_import_raw.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global detail_debugging \n",
    "detail_debugging = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Clean the data and make it consistent in the PANDAS Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_datatype_Str = str(df_event_log.dtypes)\n",
    "column_datatype_Str =  column_datatype_Str.replace('dtype: object','').replace('object','varchar[255], ').replace('datetime64[ns]','timestamp, ').replace('float64','float, ')\n",
    "print('create column SQL string:\\n', column_datatype_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = df_import_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_files.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check the data consistency and perform change control if there are differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert the pandas dataframes into SQL table Create Statements  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Creates the SQL tables in the target Database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Insert the the PANDAS Rows into SQL using the to_SQL Method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Add event logging to capture the performance of the entire process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Document the SCHEMA into an easy to use Excel Spreadsheet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Check the total number of records imported via SQL to the total raw record count to make sure no data is Left Behind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_columns = column_create_SQL(df_e_log)\n",
    "sql_columns_cleaned = ' '.join([character_replacements.get(i, i) for i in sql_columns.split()])\n",
    "sql_dtypes =  df_e_log.dtypes \n",
    "sql_column_data_types_cleaned = ' '.join([data_type_replacements.get(i, i) for i in sql_dtypes])\n",
    "\n",
    "print('SQL Columns  =',sql_columns ,' /n ' 'SQL Columns Cleaned =',sql_columns_cleaned) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_table_name_for_path(path):\n",
    "\n",
    "    table_name = path.replace(Data_Import_Starting_Directory,\"\").replace(' ','_').replace('/','').replace('\\\\','')\n",
    "    return table_name \n",
    "\n",
    "\n",
    "path = 'Y:/_Kaleida_Input/Available_Slots/'\n",
    "table_name_for_path = determine_table_name_for_path(path)\n",
    "print('Table Name:{} is determined from path:{}'.format(table_name_for_path,path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_import_all_csv_files(path):\n",
    "    \n",
    "    if importing_xlsx_files: \n",
    "        extension = 'xlsx'\n",
    "    if importing_csv_files: \n",
    "        extension = 'csv'        \n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "        add_log_event(Process_Name,'Found Table to Import',Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , 'Found Table to Import :' + file):\n",
    "   \n",
    "        \n",
    "read_and_import_all_csv_files('Y:/_Kaleida_Input/Available_Slots/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Iport Walk at Directory:',Data_Import_Starting_Directory)\n",
    "list_all_subdirectories(Data_Import_Starting_Directory)\n",
    "df_import_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def execute_table_Create_SQL(path, table_name):\n",
    "    \n",
    "    executing_SQL = False \n",
    "    \n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        if csv_file_count == 1:\n",
    "            print('Creating_Table',table_name,' based upon 1st sample','File',str(csv_file_count),\": \", file)     \n",
    "            data_folder =  path\n",
    "            filename = data_folder + table_name +'.csv'\n",
    "            print ('raw file name to rcreate from:',filename)\n",
    "            DROP_table_SQL = build_DROP_table_SQL(file, table_name,'_DI]')  \n",
    "            create_table_SQL = build_table_create_SQL(file, table_name,'_DI]')\n",
    "\n",
    "    if executing_SQL:     \n",
    "            print ('/n DROP SQL = ',DROP_table_SQL  )         \n",
    "            print ('/n create SQL = ',create_table_SQL  )     \n",
    "            execute_SQL(DROP_table_SQL)            \n",
    "            execute_SQL(create_table_SQL)\n",
    "        \n",
    "execute_table_Create_SQL('Y:/_Kaleida_Input/Access/','Access')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory): \n",
    " #print('list_sub_directories for root Ditrectory {} \\n'.format(root_directory) )   \n",
    " directory_entry = 0 \n",
    " Table_Name = ''\n",
    " print('About to Walk')\n",
    " for root,directory, file in os.walk(root_directory):\n",
    "    print('Walking....   ')\n",
    "    #print('Root Directory: {} subdir: {} \\n'.format(root_directory,directory_entry) )\n",
    "    \n",
    "    if root.find('Access') >= 0:\n",
    "        Table_Name = 'Access'\n",
    "    elif root.find('Daily Time Card') >= 0:\n",
    "        Table_Name = 'Daily_Time_Card'                \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'               \n",
    "    elif root.find('ADP') >= 0:\n",
    "        Table_Name = 'ADP'  \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'   \n",
    "    elif root.find('Available_Slots') >= 0:\n",
    "        Table_Name = 'Available_Slots'     \n",
    "    elif root.find('Available_Slots_Past') >= 0:\n",
    "        Table_Name = 'Available_Slots_Past'    \n",
    "    elif root.find('Call Center') >= 0:\n",
    "        Table_Name = 'Call_Center'    \n",
    "    elif root.find('CPT Visit') >= 0:\n",
    "        Table_Name = 'CPT_Visit'    \n",
    "    elif root.find('Visit') >= 0:\n",
    "        Table_Name = 'Visit'  \n",
    "    elif root.find('DailyAppointments') >= 0:\n",
    "        Table_Name = 'Daily_Appointments'    \n",
    "    elif root.find('DailyCPT') >= 0:\n",
    "        Table_Name = 'Daily_CPT'    \n",
    "    elif root.find('DailyMultipleAppointmentSameDay') >= 0:\n",
    "        Table_Name = 'Daily_Multiple_Appointment_Same_Day'   \n",
    "    elif root.find('DailyScheduledOfficeAppointmentVisit') >= 0:\n",
    "        Table_Name = 'Daily_Scheduled_Office_Appointment_Visit'               \n",
    "        PatientExperienceDefault\n",
    "    else:\n",
    "        Table_Name = 'Unknown Table Name'\n",
    "    print('Root Directory'+str(directory_entry) +':',root+'Default Table Name for Directory :', Table_Name,' \\n')            \n",
    "    #list_all_csv_files(root)  \n",
    "    if Table_Name != 'Unknown Table Name':\n",
    "        execute_table_Create_SQL(root,Table_Name)  \n",
    "    directory_entry += 1     \n",
    " \n",
    "# Test function call     \n",
    "list_sub_directories('Y:/_Kaleida_Input/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory):\n",
    "    df_import_directories = create_directory_dataframe() \n",
    "    print('list_sub_directories for root Directory {} \\n'.format(root_directory) )   \n",
    "    directory_entry = 0 \n",
    "    files_to_import = 0 \n",
    "    Table_Name = ''\n",
    "    for root, subdirectories, files in os.walk(root_directory):\n",
    "        directory_entry += 1 \n",
    "        print('Directory entry# {} {} .'.format(directory_entry,root ))\n",
    "        #print('Root: {} has subdirs{}'.format(root, subdirectories)\n",
    "              \n",
    "    print('End of function ')              \n",
    "    return df_import_directories\n",
    "              \n",
    "walk_sub_directories('Y:/_Kaleida_Input/')              \n",
    "\n",
    "#         for subdir in subdirectories:\n",
    "#               print('S---- Subdir: {}'.format(subdir)\n",
    "#               for file in files:\n",
    "#                    if dir_entry.find('.csv') >= 1:\n",
    "#                        files_to_import += 1 \n",
    "#                        print('F---- ----- file# {} to import: {}'.format(files_to_import,file) \n",
    "#                    else:\n",
    "#                        print('NF--- ----- file#   to NOT import: {}'.format(file)                           \n",
    "                             \n",
    "                             \n",
    "                 \n",
    "        \n",
    "        \n",
    "#         for dir_entry in directory_contents:\n",
    "#             print('Walking directory{} entry {}'.format(directory_entry, dir_entry))\n",
    "#             if os.path.isfile(dir_entry):\n",
    "#                 if dir_entry.find('.csv') >= 1:\n",
    "#                     print('File #{} to Import: {}'.format(directory_entry, dir_entry))\n",
    "             \n",
    "            \n",
    "#         row_values = [root,directory, file,'Pretend Tablename']\n",
    "#         df_import_directories.loc[directory_entry] = row_values\n",
    "\n",
    "#         print('Walking Root Directory: {} subdir: {} '.format(root_directory,directory_entry) )\n",
    "#         if root.find('Access') >= 0:\n",
    "#             Table_Name = 'Access'\n",
    "#         elif root.find('Daily Time Card') >= 0:\n",
    "#             Table_Name = 'Daily_Time_Card'                \n",
    "#         elif root.find('Employee Census') >= 0:\n",
    "#             Table_Name = 'Employee_Census'               \n",
    "#         elif root.find('ADP') >= 0:\n",
    "#             Table_Name = 'ADP'  \n",
    "#         elif root.find('Employee Census') >= 0:\n",
    "#             Table_Name = 'Employee_Census'   \n",
    "#         elif root.find('Available_Slots') >= 0:\n",
    "#             Table_Name = 'Available_Slots'     \n",
    "#         elif root.find('Available_Slots_Past') >= 0:\n",
    "#             Table_Name = 'Available_Slots_Past'    \n",
    "#         elif root.find('Call Center') >= 0:\n",
    "#             Table_Name = 'Call_Center'    \n",
    "#         elif root.find('CPT Visit') >= 0:\n",
    "#             Table_Name = 'CPT_Visit'    \n",
    "#         elif root.find('Visit') >= 0:\n",
    "#             Table_Name = 'Visit'  \n",
    "#         elif root.find('DailyAppointments') >= 0:\n",
    "#             Table_Name = 'Daily_Appointments'    \n",
    "#         elif root.find('DailyCPT') >= 0:\n",
    "#             Table_Name = 'Daily_CPT'    \n",
    "#         elif root.find('DailyMultipleAppointmentSameDay') >= 0:\n",
    "#             Table_Name = 'Daily_Multiple_Appointment_Same_Day'   \n",
    "#         elif root.find('DailyScheduledOfficeAppointmentVisit') >= 0:\n",
    "#             Table_Name = 'Daily_Scheduled_Office_Appointment_Visit'               \n",
    "#             PatientExperienceDefault\n",
    "#         else:\n",
    "#             Table_Name = 'Unknown Table Name'\n",
    "#         #print('Root Directory'+str(directory_entry) +':',root+'Default Table Name for Directory :', Table_Name,' \\n')            \n",
    "#         #list_all_csv_files(root)  \n",
    "#         if Table_Name != 'Unknown Table Name':\n",
    "#             execute_table_Create_SQL(root,Table_Name)  \n",
    "#         directory_entry += 1     \n",
    "\n",
    "#         # Test function call   \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table_create_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    "    print('Import File =', filename)   \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP the table Dynamically \n",
    "def drop_table_SQL(drop_SQL):\n",
    "    global sql_connector\n",
    "    print('Drop Table - Before SQL Connect - Call')\n",
    " #   logging.debug('Drop Table - Before SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(drop_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SQL  Dynamically \n",
    "def execute_SQL(execute_SQL_command):\n",
    "    global sql_connector\n",
    "    print('Execute SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(execute_SQL_command)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Drop_Table_SQL(Table_Name):\n",
    "    #drop_SQL = 'DROP TABLE [pbic_1_0].[' + Table_Name + ']'\n",
    "    \n",
    "    \n",
    "    drop_SQL =  'DROP TABLE [pbic_1_0].[{}]'.format(\"'\", Table_Name)\n",
    " \n",
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Access_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Access_DI]    \n",
    "    return drop_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Table_SQL = Create_Drop_Table_SQL('Access_DI')\n",
    "\n",
    "print(Drop_Table_SQL) \n",
    "#execute_SQL(Drop_Table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_file(data_folder, filename):\n",
    "\n",
    "    print('Import File =', filename)                 \n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    #print(df_input_csv.columns)\n",
    "    df_input_csv.columns = df_input_csv.columns.map('^'.join)\n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "\n",
    "    print(df_input_csv.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Dataframe_table_create_SQL(dataframe_to_Create, table_name, table_Postfix  ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    " \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    \n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"Address 1\",\"Street_Address\").replace(\"Address 2\",\"Address_Two\") \\\n",
    "                             .replace(\"^\",\"\").replace(\"-\",\"_\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\") \\\n",
    "                             .replace(\"%\",\"Percentage\").replace('_Unnamed','').replace('Unnamed','') \\\n",
    "                             .replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "    \n",
    "         \n",
    "            \n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DROP_table_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    DROP_table_SQL  = 'DROP Table ' + Table_Schema_Prefix + Table_Name  \n",
    "    return DROP_table_SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "\n",
    "print(\"\\nDaily Table Create SQL: \\n\" + create_table_SQL)\n",
    "\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_HX]')\n",
    "\n",
    "print(\"\\nHistorical Table Create SQL: \\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y:/_Kaleida_Input/Access/2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments')\n",
    "\n",
    "print(\"\\n\" + create_table_SQL)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Daily_Appointments_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Daily_Appointments_DI]\n",
    "\n",
    "\n",
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_SQL(create_table_SQL)\n",
    "print(\"\\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "# filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    " \n",
    "read_and_clean_file(data_folder,filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_table_headers(input_data_frame):\n",
    "    global column_inserts\n",
    "    global column_question_mark\n",
    "    global create_table_SQL\n",
    "    global create_real_table_SQL\n",
    "    global insert_records_SQL\n",
    "    global Table_Name_Prefix\n",
    "    global Table_Name_Extension_Daily\n",
    "    global Table_Name_Extension_Historical\n",
    "    global create_schema_SQL\n",
    "    global create_real_schema_SQL\n",
    "        \n",
    "    Table_Name = 'Access'\n",
    "    sample_row = 3\n",
    "    create_table_SQL = ''\n",
    "    create_real_table_SQL = ''\n",
    "    insert_records_SQL = ''\n",
    "    Table_Name_Daily = Table_Name_Prefix + Table_Name + Table_Name_Extension_Daily\n",
    "    Table_Name_Historical = Table_Name_Prefix + Table_Name + Table_Name_Extension_Historical\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes\n",
    "    col_number = 0\n",
    "    column_creates = ''\n",
    "    column_values = ''\n",
    "    column_inserts = ''\n",
    "    real_column_creates = ''\n",
    "    column_question_mark = ''\n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            real_sql_column_type = 'Varchar(255)'                \n",
    "                \n",
    "\n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name\n",
    "        column_value = str(input_data_frame.iloc[sample_row,col_number-1])\n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type\n",
    "        real_column_creates = real_column_creates + column_name + \" \"  + real_sql_column_type\n",
    "        column_values = column_values + \"'\" + column_value + \"'\"\n",
    "        print(col_number, '  ', column_name)\n",
    "            \n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name_Daily + '  (' + column_inserts + ') + VALUES (' + column_values + '); '\n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name_Daily + '  (' + column_creates + '); '\n",
    "    create_real_table_SQL = 'CREATE TABLE ' + Table_Name_Historical + '  (' + real_column_creates + '); '\n",
    "    create_schema_SQL = create_schema_SQL + create_table_SQL\n",
    "    create_real_schema_SQL = create_real_schema_SQL + create_real_table_SQL\n",
    "    #logging.debug('Table Create Finished')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'Y:\\_Kaleida_Input\\Access\\2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "start_time1 = time.time()\n",
    "\n",
    "data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "print('Import File =', filename)                 \n",
    "df_input_csv = pd.read_csv(filename, nrows=10, header=[0,1])\n",
    "df_input_csv.columns = df_input_csv.columns.map('_'.join)\n",
    "create_table_headers(df_input_csv) \n",
    "\n",
    "\n",
    "#print('\\n' + ' column_inserts:  ', column_inserts, '\\n') \n",
    "#print('\\n' + 'column_question_mark:  ', column_question_mark, '\\n') \n",
    "#print('\\n' + 'insert_records_SQL:  ', insert_records_SQL, '\\n')\n",
    "print('\\n' + 'create_table_SQL:  ', create_table_SQL, '\\n')\n",
    "print('\\n' + 'create_real_table_SQL:  ', create_real_table_SQL, '\\n')\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyappointment_df = pd.read_csv(r'C:\\DailyAppointment_A_J_Test\\Main 3.1 to 9.1.xlsx', low_memory = False, header = [0,1])\n",
    "Main 3.1 to 9.1.xlsx\n",
    "\n",
    "Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\n",
    "dailyappointment_df.columns = dailyappointment_df.columns.map('_'.join)\n",
    "dailyappointment_df = dailyappointment_df.fillna(0)\n",
    "dailyappointment_df['Unnamed: 3_Appt Length'] = dailyappointment_df['Unnamed: 3_Appt Length'].astype(int)\n",
    "#...\n",
    "engine = sqlalchemy.create_engine(\n",
    "               \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\",\n",
    "               echo=False)\n",
    "# # df = pd.read_sql_query('SELECT * FROM pbic_1_0.Access',conn)\n",
    "import time \n",
    "start_time1 = time.time()\n",
    "#dailyappointment_df.to_sql('dailyappointment_test', con=engine, if_exists='replace')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time1 = time.time()\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 2.28 to 8.28.xlsx' )\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.2 to 9.2.xlsx' )\n",
    "\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.2 to 8.2.xlsx' )\n",
    "\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.2.xlsx' )\n",
    "\n",
    "Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "Available_Slots_df.rename(columns={'Doctor Name' : 'Doctor_Name','Loc UId' : 'Loc_UId'}, inplace = True)\n",
    " \n",
    "shape = Available_Slots_df.shape\n",
    "print('\\nDataFrame Shape :', shape)\n",
    "print('\\nNumber of rows :', shape[0])\n",
    "print('\\nNumber of columns :', shape[1])\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "file_read_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('Read raw file to Pandas Read Time',f'{file_read_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n",
    "create_table_headers(Available_Slots_df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    \n",
    "    extension = 'xlsx'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "list_all_csv_files('Z:/GPPC_SOURCE_FILES/Oneday_data4_1_22/Available_Slots/')   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "\n",
    "Available_Slots_df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=20000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "\n",
    "\n",
    "# shape = Available_Slots_df.shape\n",
    "# print('\\nDataFrame Shape :', shape)\n",
    "# print('\\nNumber of rows :', shape[0])\n",
    "# print('\\nNumber of columns :', shape[1])\n",
    " \n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "execute_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('SQL Insert Execution Time',f'{execute_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_headers(Available_Slots_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE [pbic_1_0].[Available_Slots](\n",
    "\t[Date] [nvarchar](20) NULL,\n",
    "\t[Day] [nvarchar](50) NULL,\n",
    "\t[Time] [nvarchar](20) NULL,\n",
    "\t[Length] [int] NULL,\n",
    "\t[Dr] [nvarchar](50) NULL,\n",
    "\t[Doctor_Name] [nvarchar](max) NULL,\n",
    "\t[Loc] [nvarchar](50) NULL,\n",
    "\t[Loc_UId] [nvarchar](50) NULL,\n",
    "\t[Type] [nvarchar](50) NULL\n",
    ") ON [PRIMARY] TEXTIMAGE_ON [PRIMARY]\n",
    "GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Available_Slots_df.info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "drop_table_SQL(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[dailyappointment_test]'\n",
    "drop_SQL_table(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Hx', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype = \n",
    "{'datefld': sqlalchemy.DateTime(), \n",
    "'intfld': sqlalchemy.types.INTEGER(),\n",
    "'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "'booleanfld': sqlalchemy.types.Boolean,\n",
    "'bool' : sqlalchemy.types.Boolean,\n",
    "'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "'int64' : sqlalchemy.types.INTEGER(),\n",
    "'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global Process_Name \n",
    "\n",
    "today = date.today()\n",
    "\n",
    "start_time1 = time.time() \n",
    "time.sleep(3)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Starting Import Process',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Starting Import Process\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Reading the CSV files',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Reading the CSV filesS\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(1)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Writing to SQL Server',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Writing to SQL Server\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Import Process END ',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Import Process END\")\n",
    "\n",
    "df_e_log.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rows(insert_SQL):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the parent directory for all the data subdirectories \n",
    "parent_dir = 'Y:/_Kaleida_Input/' #path to folder that contians the data folders\n",
    "\n",
    "path = parent_dir\n",
    "import_file_type = '\\*.csv'\n",
    "create_table_SQL = ''\n",
    "insert_records_SQL = '' \n",
    "create_schema_SQL = '' \n",
    "column_inserts  = ''\n",
    "column_question_mark  = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_import_files(data_directory_path, import_file_type ):\n",
    "    all_files = glob.glob(data_directory_path + import_file_type)\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "   ...: print('Insert SQL: ', insert_SQL)\n",
    "   ...: insert_row_SQL(insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_headers(input_data_frame):\n",
    "    global column_inserts  \n",
    "    global column_question_mark  \n",
    "    global create_table_SQL \n",
    "    global insert_records_SQL \n",
    "\n",
    "    Table_Name = 'Access'\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes \n",
    "    col_number = 0 \n",
    "    column_inserts = '' \n",
    "    column_creates = '' \n",
    "    column_question_mark = '' \n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "        if len(column_inserts) > 1:\n",
    "            column_inserts = column_inserts + \", \"\n",
    "        if len(column_creates) > 1:\n",
    "            column_creates = column_creates + \", \"     \n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64':  \n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name \n",
    "        \n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type \n",
    "        column_question_mark = column_question_mark + \"?, \"\n",
    "        print(col_number, '  ', column_name) \n",
    "    #print('column_inserts:  ', column_inserts) \n",
    "    #print('column_question_mark:  ', column_question_mark) \n",
    "    print('column_creates:  ', column_creates)  \n",
    "\n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name + '(' + column_inserts + ') VALUES (' + column_question_mark + ');' \n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name + '(' + column_creates + ');' \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = iterate_import_files('Y:\\_Kaleida_Input\\Access','\\\\*.csv')\n",
    "# print(all_files[1])\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, nrows=10)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('C:\\Data\\Behavioral Health'+ '\\*.csv')\n",
    "print(all_files)\n",
    "print(all_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:41:00.444921Z",
     "start_time": "2022-08-30T12:40:48.868620Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove all csv files from dir and unzip folder\n",
    "parent_dir = 'C:/Power BI/' #path to folder\n",
    "path = parent_dir\n",
    "\n",
    "#get csv list\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        os.remove(path+file)    \n",
    "        \n",
    "with ZipFile(path+'PowerBiDownload.zip', 'r') as zipObj:\n",
    "   zipObj.extractall(path)   \n",
    "\n",
    "#remove files\n",
    "#remove files not in list\n",
    "csv_path = r'S:\\Data Team\\Source Data\\python sql\\needed tables.csv'\n",
    "ext = \".csv\"\n",
    "with open(csv_path, 'r') as csvfile:\n",
    "    good_files = []\n",
    "    for n in csv.reader(csvfile):\n",
    "        if len(n) > 0: good_files.append(n[0])\n",
    "    all_files = os.listdir(path)\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(ext) and filename not in good_files:\n",
    "            full_file_path = os.path.join(path, filename)\n",
    "            os.remove(full_file_path)\n",
    "\n",
    "print('Old files removed, new files unzipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:44:43.698446Z",
     "start_time": "2022-08-30T12:41:04.155886Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pull in helper tables, covert to csv and delete old helper tables\n",
    "with open('S:/Data Team/Source Data/python sql/helper tables.csv', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in linereader:\n",
    "        name = row[0]\n",
    "        shutil.copy(name, 'C:\\Power BI\\\\' + os.path.basename(name))\n",
    "                \n",
    "print('All Helper Tables Moved')\n",
    "\n",
    "searchdir = 'C:\\Power BI\\\\'\n",
    "\n",
    "for xls_file in glob.glob(os.path.join(searchdir,\"*.xlsx\")):\n",
    "    data_xls = pd.read_excel(xls_file, index_col = None)\n",
    "    csv_file = os.path.splitext(xls_file)[0]+\".csv\"\n",
    "    data_xls.to_csv(csv_file, encoding = 'utf-8', index = False)\n",
    "    \n",
    "print('All Helper Tables Changed to CSV')\n",
    "\n",
    "df_pipe = pd.read_csv('C:/Power BI/hec daily.txt', delimiter = '|', index_col = None, header = None, on_bad_lines='skip')\n",
    "\n",
    "df_pipe.to_csv('C:/Power BI/hec daily.csv', sep = ',', header = False, index = False)    \n",
    "    \n",
    "print('HeC Daily Converted to CSV')\n",
    "\n",
    "pathtodelete =r\"C:\\Power BI\"\n",
    "filenames_xlsx = glob.glob(pathtodelete + \"/*.xlsx\")\n",
    "for i in filenames_xlsx:\n",
    "    os.remove(i)\n",
    "    \n",
    "filenames_txt = glob.glob(pathtodelete + \"/*.txt\")\n",
    "for k in filenames_txt:\n",
    "    os.remove(k)    \n",
    "    \n",
    "print('Old Helper Tables Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:27:48.939756Z",
     "start_time": "2022-08-30T13:27:35.810299Z"
    }
   },
   "outputs": [],
   "source": [
    "#rename files longer than >=63 char\n",
    "for filename in os.listdir(path):\n",
    "    if len(filename) > 63:\n",
    "        os.rename(path+filename, path+filename[-60:])\n",
    "        print(filename+' renamed to '+filename[-60:])\n",
    "            \n",
    "#get csv list\n",
    "csv_files = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)\n",
    "        \n",
    "data_path = path\n",
    "#create dataframes\n",
    "df = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(data_path+file, low_memory=False, index_col=False)\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        df[file] = pd.read_csv(data_path+file, encoding=\"cp437\", low_memory=False, index_col=False, errors='ignore')\n",
    "    \n",
    "    print('Loading ' + file + ' into dataframe')    \n",
    "print('loading completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:34:56.034850Z",
     "start_time": "2022-08-30T13:28:02.768562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in csv_files:\n",
    "    \n",
    "    dataframe = df[k]\n",
    "    \n",
    "    clean_tbl_name = k.lower().replace(\" \",\"_\").replace(\"-\",\"\").replace(\".\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\\\n",
    "    .replace(\"_csv\", \"\").replace(\"___\", \"_\").replace(\"__\", \"_\")    \n",
    "    \n",
    "    tbl_name = clean_tbl_name\n",
    "    \n",
    "    print(k + ' changing to ' + clean_tbl_name)\n",
    "\n",
    "    #clean column names\n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"-\", \"\").replace(\"#\",\"num\").replace(\"?\", \"\")\\\n",
    "                     .replace(\"=\",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\r\\n\",\"\").replace(\"]\",\"_\")\\\n",
    "                     .replace(\"]\",\"_\").replace(\"[\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\")\\\n",
    "                     .replace(\"%\",\"\").replace(\"#\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"?\",\"\")\\\n",
    "                     .replace(\",\",\"\").replace(\"*\",\"\").replace(\":\",\"\").replace(\"'\",\"\").replace(\"&\",\"\")\\\n",
    "                     .replace(\";\",\"\").replace(\"__\", \"_\").replace(\"/\", \"\")\n",
    "                     for x in dataframe.columns]\n",
    "\n",
    "     #limit column length to 64 and reading right to left\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "\n",
    "     #adding a number if duplicated column name\n",
    "    def uniquify(dataframe):\n",
    "        seen = set()\n",
    "\n",
    "        for item in dataframe:\n",
    "            fudge = 1\n",
    "            newitem = item\n",
    "\n",
    "            while newitem in seen:\n",
    "                fudge += 1\n",
    "                newitem = \"{}_{}\".format(item, fudge)\n",
    "\n",
    "            yield newitem\n",
    "            seen.add(newitem)\n",
    "\n",
    "    dataframe.columns = uniquify(dataframe)\n",
    "\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "    \n",
    "    #db settings and connection\n",
    "    #get password\n",
    "    f=open(\"S:/Data Team Secure/secrets/postgres.txt\",\"r\")\n",
    "    lines=f.readlines()\n",
    "    password=lines[1]\n",
    "    f.close()\n",
    "    \n",
    "   \n",
    "    user=\"Joes_User_Name\"\n",
    "    host = 'Joes_Host_Name'\n",
    "    dbname = 'postgres'\n",
    "        \n",
    "    engine = create_engine('postgresql://'+user+':'+password+'@'+host+'/'+dbname)\n",
    "   \n",
    "    #print('opened database successfully')\n",
    "    \n",
    "    #create table\n",
    "    #dataframe.to_sql(k, engine, schema = None, if_exists='append', index=False, dtype = 'text')\n",
    "    dataframe.to_sql(clean_tbl_name, engine, schema = None, if_exists='append', index=False, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "    \n",
    "    print(clean_tbl_name+' uploaded to database')\n",
    "    \n",
    "print('All uploads complete')\n",
    "\n",
    "## for automation, send email to email list when complete\n",
    "#get email and file list\n",
    "email_list = pd.read_csv('S:/Data Team/Source Data/python sql/email_db_upload.csv')\n",
    "emails = email_list['email']\n",
    "\n",
    "# email loop\n",
    "for i in range(len(emails)):\n",
    "    \n",
    "    email = emails[i]\n",
    "    \n",
    "    # Open the Outlook\n",
    "    outlook = win32.Dispatch('outlook.application')\n",
    "\n",
    "    # Create the email\n",
    "    mail = outlook.CreateItem(0)\n",
    "\n",
    "    # Set the email subject\n",
    "    mail.Subject = 'AUTOMATED EMAIL: Database Updated '+ datetime.now().strftime('%b %#d %Y %H:%M')\n",
    "\n",
    "    # Set the receiver email\n",
    "    mail.To = email\n",
    "\n",
    "    # Write the email content\n",
    "    mail.HTMLBody = r\"\"\"\n",
    "    <p>Hello</p>\n",
    "    <p>The database has had been updated successfully.</p>\n",
    "    <p>Thanks</p>\n",
    "    <p>The Data Team</p>\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the email\n",
    "    mail.Send()\n",
    "    print('Email sent to ' + email)\n",
    "print('All Emails Processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

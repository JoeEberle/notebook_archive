{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47b9f41",
   "metadata": {},
   "source": [
    "# Bulk Importing of CSV files\n",
    "#### Created by Joe Eberle, Alan Calhoun, Helmi (Al)  Seoud     Refactored ON  : 10/17/2022  ---  Revised ON  : 10/18/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80896d8",
   "metadata": {},
   "source": [
    "## Project Setup - Importing Libraries and Initializing Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc38cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependent Libraries is not already installed \n",
    "#!pip install pyttsx3\n",
    "\n",
    "# Import the necessary Libraries \n",
    "import glob, os\n",
    "import pandas as pd\n",
    "# import logging \n",
    "from pathlib import Path\n",
    "import pyttsx3\n",
    "import pyodbc \n",
    "import timeit\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sqlalchemy\n",
    "\n",
    "# Establish some import parameters \n",
    "\n",
    "importing_xlsx_files = False \n",
    "importing_csv_files = True \n",
    "Data_Import_Starting_Directory = 'Y:/_Kaleida_Input/'\n",
    "#Data_Import_Starting_Directory = 'C:/Data/'\n",
    "Process_Name = 'Importing CSV data into SQL'\n",
    "\n",
    "step_debugging = True\n",
    "detail_debugging = True\n",
    "detail_Talking = False # only talk on major steps \n",
    "Process_Step_Name = ''  \n",
    "Reading_Intro = False\n",
    "Reading_Credits =  False\n",
    "Reading_Steps = False \n",
    "Reading_Terms = False \n",
    "printing_output = True\n",
    "Talking_Code = True\n",
    "Talking_Voice_Male_Gender = True        # Set to False for Female Voice \n",
    "Code_Logging  = True \n",
    "event_log_row = 0 \n",
    "\n",
    "# Create some Global Variables for SQL Constructs \n",
    "column_inserts = ''\n",
    "column_question_mark   = ''\n",
    "create_table_SQL  = ''\n",
    "create_real_table_SQL  = ''\n",
    "insert_records_SQL  = ''\n",
    "create_schema_SQL  = '' \n",
    "create_real_schema_SQL  = '' \n",
    "Table_Name_Extension_Daily = '_DI'\n",
    "Table_Name_Extension_Historical = '_HX'\n",
    "Table_Name_Extension_Rejected = '_RJ'\n",
    "Table_Name_Extension_Administrative = '_AD'\n",
    "Table_Name_Prefix = '[pbic_1_0].'\n",
    "\n",
    "# Create some Global Variables for SQL Connection\n",
    "server = 'Kalpwvsqlgppc01' \n",
    "database  = 'GPPC_DEV' \n",
    "username ='GPPC'\n",
    "pwd = 'Elephant-Trunk-06'\n",
    "sql_connector = 'DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd\n",
    "# log_filename='data_importing_log.log'\n",
    "\n",
    "# Configure the Logging to the DEBUG Level \n",
    "# logging.basicConfig(level=logging.DEBUG, filename=log_filename, format= '%(asctime)s %(clientip)-15s %(user)-8s %(message)s')\n",
    "Text_to_Speech = pyttsx3.init()\n",
    "\n",
    "## Dictionary For character_replacements List \n",
    "character_replacements = { \" \":\"_\",\"#\":\"Number\",\"%\":\"Percentage\" \\\n",
    "                         ,'_Unnamed':'','_Level':'',\"$\":\"Dollar\",'_1':'' \\\n",
    "                         ,'_2':'','_3':'','_4':'','_5':''  \\\n",
    "                         ,'_6':'','_7':'','_8':'','_9':''  \\\n",
    "                         ,'_0':'',':7':'',':8':'',':':'' }\n",
    "## Dictionary For replacing data types in databases \n",
    "data_type_replacements = { \"object\":\"varchar\",\"float64\":\"float\",\"int64\":\"int\",\"%\":\"Percentage\" \\\n",
    "                         ,'_Unnamed':'','datetime64':'timestamp',\"timedelta64[ns]\":\"varchar\"}    \n",
    "\n",
    "## add the glaobal data frames for event loggging and Schema Creation \n",
    "df_event_log = pd.DataFrame(columns = ('Event_ID','Process_Name','Event_Name','Event_Date','Event_Time','Task_Start_Time','Task_End_Time','TASk_Duration','Comments'))\n",
    "df_import_directories = pd.DataFrame(columns = ('Root_Directory','Sub_Directory'))\n",
    "df_import_files = pd.DataFrame(columns = ('Root_Directory','Sub_Directory','Table_Name','Import_File_Name','File_Size','File_Modified_Date','File_Created_Date'))\n",
    "import_directory_file_Number  = 0\n",
    "import_file_Number  = 0 \n",
    "event_log_row = 0\n",
    "sub_directory_count = 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e65dd",
   "metadata": {},
   "source": [
    "## Project Setup -    Database Connectivity & SQL Generation & SQL Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868c6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SQL  Dynamically based upon the Connection string  \n",
    "def execute_SQL(execute_SQL_command):\n",
    "    global sql_connector\n",
    "    print('Execute SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(execute_SQL_command)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    out('Executing SQL - After SQL Execute - Command: {} '.format(execute_SQL_command))    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()\n",
    "    \n",
    "# The following code uses a SQL server template for Droping tables and replaces the table name \n",
    "# spo that it will create the SQL code will drop ANY Table name that is passed to it \n",
    "def Create_Drop_Table_SQL(table_name):  \n",
    "    # Example ---- DROP TABLE [pbic_1_0].[Access_DI]\n",
    "    Drop_Table_SQL  = 'DROP TABLE [pbic_1_0].[{}]'.format(table_name)\n",
    "    return Drop_Table_SQL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89351e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files to import :1914\n"
     ]
    }
   ],
   "source": [
    "# read the registry of files from the dayta discovery process \n",
    "Data_Import_File_Registry =  'J:/IT GLIN Data Services Shared/TempData/'\n",
    "Excel_file_Name = Data_Import_File_Registry + 'Discovered_CSV_files_to_import.xlsx'\n",
    "df_import_files = pd.read_excel(Excel_file_Name)\n",
    "print('Number of Files to import :' + str(df_import_files.shape[0]) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4794932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_the_table_files_up(table_Name):\n",
    "    df_raw_table_files  = df_import_files[(df_import_files.Table_Name == table_Name)]  # Slice the data set down to one table \n",
    "    sub_dir = df_raw_table_files.Sub_Directory.unique() # Determine the subdirectory to read from \n",
    "    sub_directory= sub_dir[0] \n",
    "    \n",
    "    raw_csv_files = df_raw_table_files.Import_File_Name.unique()\n",
    "    for file  in raw_csv_files:\n",
    "        print('Table: {} has a raw file:{} to import from dir: {} '.format(table_Name,file, sub_directory))\n",
    "        fullpath_filename = str(sub_directory) +  str('/') + str(file) \n",
    "        print('File to CSV READ : {} '.format(fullpath_filename))        \n",
    "        df_csv = pd.read_csv(fullpath_filename, nrows = 10)\n",
    "        df_csv.head()\n",
    "        print('Rows:' + str(df_csv.shape[0]) + ' Columns:' + str(df_csv.shape[1])) \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db6d73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Names to Import :\n",
      " ['Time_Card' 'Employee_Census' 'Available_Slots' 'Appointments'\n",
      " 'Historical' 'Patient_Information' 'Primary_Care_Annual_Visit'\n",
      " 'RCM_Adjustments' 'RCM_Productivity' 'Surgical_Appointment'\n",
      " 'Third_Next_Available'] \n",
      "\n",
      "Number of Tables to Import : 11 \n"
     ]
    }
   ],
   "source": [
    "tables_to_import = df_import_files.Table_Name.unique() # Pull the unique Table names out of the registry\n",
    "print('\\nTable Names to Import :\\n {} \\n\\nNumber of Tables to Import : {} '.format(tables_to_import,str(len(tables_to_import))))\n",
    "# for table in tables_to_import:\n",
    "#     slice_the_table_files_up(table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69418a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:100 Columns:28\n"
     ]
    }
   ],
   "source": [
    "df_csv = pd.read_csv('Y:/_Kaleida_Input/ADP\\\\Daily Time Card/DailyTimecardReport.050421190755.csv', nrows = 100 )\n",
    "print('Rows:' + str(df_csv.shape[0]) + ' Columns:' + str(df_csv.shape[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deb71041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Payroll Company Code</th>\n",
       "      <th>Position ID</th>\n",
       "      <th>Payroll Name</th>\n",
       "      <th>Business Unit Description</th>\n",
       "      <th>Home Department Description</th>\n",
       "      <th>Location Description</th>\n",
       "      <th>Timecard Pay Date</th>\n",
       "      <th>REGULAR</th>\n",
       "      <th>OVERTIME</th>\n",
       "      <th>PTO-unscheduled</th>\n",
       "      <th>...</th>\n",
       "      <th>FLOAT</th>\n",
       "      <th>CME</th>\n",
       "      <th>FMLA</th>\n",
       "      <th>SICK</th>\n",
       "      <th>CSL</th>\n",
       "      <th>PFL</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>TOV</th>\n",
       "      <th>90D</th>\n",
       "      <th>OTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002752</td>\n",
       "      <td>Abdel-Aal Ahmed, Mohamed</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>CV_BGH B2_OUTPATIENT CLINIC</td>\n",
       "      <td>CV_BGH B2</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002752</td>\n",
       "      <td>Abdel-Aal Ahmed, Mohamed</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>CV_BGH B2_OUTPATIENT CLINIC</td>\n",
       "      <td>CV_BGH B2</td>\n",
       "      <td>11/26/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002752</td>\n",
       "      <td>Abdel-Aal Ahmed, Mohamed</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>CV_BGH B2_OUTPATIENT CLINIC</td>\n",
       "      <td>CV_BGH B2</td>\n",
       "      <td>12/25/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002752</td>\n",
       "      <td>Abdel-Aal Ahmed, Mohamed</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>CV_BGH B2_OUTPATIENT CLINIC</td>\n",
       "      <td>CV_BGH B2</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/14/2020</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/15/2020</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/16/2020</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/17/2020</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/18/2020</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/19/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/21/2020</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/22/2020</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/23/2020</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/24/2020</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/25/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/26/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/28/2020</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/29/2020</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/30/2020</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>G4H</td>\n",
       "      <td>G4H002937</td>\n",
       "      <td>Abdelhaq, Dowah</td>\n",
       "      <td>General Surgery</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>DSG_WLMS_YOUNGS</td>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Payroll Company Code Position ID              Payroll Name  \\\n",
       "0                   G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
       "1                   G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
       "2                   G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
       "3                   G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
       "4                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "5                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "6                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "7                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "8                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "9                   G4H   G4H002937           Abdelhaq, Dowah   \n",
       "10                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "11                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "12                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "13                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "14                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "15                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "16                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "17                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "18                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "19                  G4H   G4H002937           Abdelhaq, Dowah   \n",
       "\n",
       "   Business Unit Description  Home Department Description  \\\n",
       "0                 Cardiology  CV_BGH B2_OUTPATIENT CLINIC   \n",
       "1                 Cardiology  CV_BGH B2_OUTPATIENT CLINIC   \n",
       "2                 Cardiology  CV_BGH B2_OUTPATIENT CLINIC   \n",
       "3                 Cardiology  CV_BGH B2_OUTPATIENT CLINIC   \n",
       "4            General Surgery              DSG_WLMS_YOUNGS   \n",
       "5            General Surgery              DSG_WLMS_YOUNGS   \n",
       "6            General Surgery              DSG_WLMS_YOUNGS   \n",
       "7            General Surgery              DSG_WLMS_YOUNGS   \n",
       "8            General Surgery              DSG_WLMS_YOUNGS   \n",
       "9            General Surgery              DSG_WLMS_YOUNGS   \n",
       "10           General Surgery              DSG_WLMS_YOUNGS   \n",
       "11           General Surgery              DSG_WLMS_YOUNGS   \n",
       "12           General Surgery              DSG_WLMS_YOUNGS   \n",
       "13           General Surgery              DSG_WLMS_YOUNGS   \n",
       "14           General Surgery              DSG_WLMS_YOUNGS   \n",
       "15           General Surgery              DSG_WLMS_YOUNGS   \n",
       "16           General Surgery              DSG_WLMS_YOUNGS   \n",
       "17           General Surgery              DSG_WLMS_YOUNGS   \n",
       "18           General Surgery              DSG_WLMS_YOUNGS   \n",
       "19           General Surgery              DSG_WLMS_YOUNGS   \n",
       "\n",
       "   Location Description Timecard Pay Date  REGULAR  OVERTIME  PTO-unscheduled  \\\n",
       "0             CV_BGH B2        09/07/2020     0.00       0.0              0.0   \n",
       "1             CV_BGH B2        11/26/2020     0.00       0.0              0.0   \n",
       "2             CV_BGH B2        12/25/2020     0.00       0.0              0.0   \n",
       "3             CV_BGH B2        01/01/2021     0.00       0.0              0.0   \n",
       "4       DSG_WLMS_YOUNGS        12/14/2020     6.50       0.0              0.0   \n",
       "5       DSG_WLMS_YOUNGS        12/15/2020     6.00       0.0              0.0   \n",
       "6       DSG_WLMS_YOUNGS        12/16/2020     8.00       0.0              0.0   \n",
       "7       DSG_WLMS_YOUNGS        12/17/2020     6.00       0.0              0.0   \n",
       "8       DSG_WLMS_YOUNGS        12/18/2020     7.00       0.0              0.0   \n",
       "9       DSG_WLMS_YOUNGS        12/19/2020     0.00       0.0              0.0   \n",
       "10      DSG_WLMS_YOUNGS        12/21/2020     7.00       0.0              0.0   \n",
       "11      DSG_WLMS_YOUNGS        12/22/2020     8.00       0.0              0.0   \n",
       "12      DSG_WLMS_YOUNGS        12/23/2020     8.50       0.0              0.0   \n",
       "13      DSG_WLMS_YOUNGS        12/24/2020     6.75       0.0              0.0   \n",
       "14      DSG_WLMS_YOUNGS        12/25/2020     0.00       0.0              0.0   \n",
       "15      DSG_WLMS_YOUNGS        12/26/2020     0.00       0.0              0.0   \n",
       "16      DSG_WLMS_YOUNGS        12/28/2020     7.50       0.0              0.0   \n",
       "17      DSG_WLMS_YOUNGS        12/29/2020     9.00       0.0              0.0   \n",
       "18      DSG_WLMS_YOUNGS        12/30/2020     8.75       0.0              0.0   \n",
       "19      DSG_WLMS_YOUNGS        12/31/2020     0.00       0.0              0.0   \n",
       "\n",
       "    ...  FLOAT  CME  FMLA  SICK  CSL  PFL  OTHER  TOV  90D  OTH  \n",
       "0   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "1   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "2   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "3   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "4   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "5   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "6   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "7   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "8   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "9   ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "10  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "11  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "12  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "13  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "14  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "15  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "16  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "17  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "18  ...    0.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "19  ...    8.0  0.0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9de9885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import File = Y:/_Kaleida_Input/ADP\\Daily Time Card/DailyTimecardReport.050421190755.csv\n",
      "Header:  Index(['Payroll Company Code', 'Position ID', 'Payroll Name',\n",
      "       'Business Unit Description', 'Home Department Description',\n",
      "       'Location Description', 'Timecard Pay Date', 'REGULAR', 'OVERTIME',\n",
      "       'PTO-unscheduled', 'PTO', 'PTO BANK', 'PTO-UNSCHEDULED', 'REGSAL',\n",
      "       'UNPAID LEAVE', 'HOLIDAY', 'JURY', 'BEREAV', 'FLOAT', 'CME', 'FMLA',\n",
      "       'SICK', 'CSL', 'PFL', 'OTHER', 'TOV', '90D', 'OTH'],\n",
      "      dtype='object')\n",
      "row1_columns:    Payroll Company Code Position ID              Payroll Name  \\\n",
      "0                  G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
      "\n",
      "  Business Unit Description  Home Department Description Location Description  \\\n",
      "0                Cardiology  CV_BGH B2_OUTPATIENT CLINIC            CV_BGH B2   \n",
      "\n",
      "  Timecard Pay Date  REGULAR  OVERTIME  PTO-unscheduled  ...  FLOAT  CME  \\\n",
      "0        09/07/2020      0.0       0.0              0.0  ...    0.0  0.0   \n",
      "\n",
      "   FMLA  SICK  CSL  PFL  OTHER  TOV  90D  OTH  \n",
      "0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1 rows x 28 columns]\n",
      "Create SQL is:\n",
      "\n",
      "\n",
      "Create Table [pbic_1_0].[Time_Card_DI(Payroll Company Code object, Position ID object, Payroll Name object, Business Unit Description object, Home Department Description object, Location Description object, Timecard Pay Date object, REGULAR float64, OVERTIME float64, PTO-unscheduled float64, PTO float64, PTO BANK float64, PTO-UNSCHEDULED float64, REGSAL float64, UNPAID LEAVE float64, HOLIDAY float64, JURY float64, BEREAV float64, FLOAT float64, CME float64, FMLA float64, SICK float64, CSL float64, PFL float64, OTHER float64, TOV float64, 90D float64, OTH float64); \n"
     ]
    }
   ],
   "source": [
    "Create_SQL = build_table_create_SQL('Y:/_Kaleida_Input/ADP\\\\Daily Time Card/DailyTimecardReport.050421190755.csv','Time_Card','_DI') \n",
    "print('Create SQL is:\\n\\n')\n",
    "print(Create_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc94db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_import_raw = customize_column_name_clean_up(df_import_raw)\n",
    "df_import_raw.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global detail_debugging \n",
    "detail_debugging = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed4d43",
   "metadata": {},
   "source": [
    "##  Step 4: Clean the data and make it consistent in the PANDAS Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfead606",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_datatype_Str = str(df_event_log.dtypes)\n",
    "column_datatype_Str =  column_datatype_Str.replace('dtype: object','').replace('object','varchar[255], ').replace('datetime64[ns]','timestamp, ').replace('float64','float, ')\n",
    "print('create column SQL string:\\n', column_datatype_Str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = df_import_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_files.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50e9d0",
   "metadata": {},
   "source": [
    "## Step 5: Check the data consistency and perform change control if there are differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a44144",
   "metadata": {},
   "source": [
    "## Step 6: Convert the pandas dataframes into SQL table Create Statements  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd2dd4",
   "metadata": {},
   "source": [
    "## Step 7: Creates the SQL tables in the target Database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf88d6",
   "metadata": {},
   "source": [
    "## Step 8: Insert the the PANDAS Rows into SQL using the to_SQL Method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7da604",
   "metadata": {},
   "source": [
    "## Step 9: Add event logging to capture the performance of the entire process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a9951",
   "metadata": {},
   "source": [
    "## Step 10: Document the SCHEMA into an easy to use Excel Spreadsheet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319adc4",
   "metadata": {},
   "source": [
    "## Step 11: Check the total number of records imported via SQL to the total raw record count to make sure no data is Left Behind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78476851",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_columns = column_create_SQL(df_e_log)\n",
    "sql_columns_cleaned = ' '.join([character_replacements.get(i, i) for i in sql_columns.split()])\n",
    "sql_dtypes =  df_e_log.dtypes \n",
    "sql_column_data_types_cleaned = ' '.join([data_type_replacements.get(i, i) for i in sql_dtypes])\n",
    "\n",
    "print('SQL Columns  =',sql_columns ,' /n ' 'SQL Columns Cleaned =',sql_columns_cleaned) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf68f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_table_name_for_path(path):\n",
    "\n",
    "    table_name = path.replace(Data_Import_Starting_Directory,\"\").replace(' ','_').replace('/','').replace('\\\\','')\n",
    "    return table_name \n",
    "\n",
    "\n",
    "path = 'Y:/_Kaleida_Input/Available_Slots/'\n",
    "table_name_for_path = determine_table_name_for_path(path)\n",
    "print('Table Name:{} is determined from path:{}'.format(table_name_for_path,path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_import_all_csv_files(path):\n",
    "    \n",
    "    if importing_xlsx_files: \n",
    "        extension = 'xlsx'\n",
    "    if importing_csv_files: \n",
    "        extension = 'csv'        \n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "        add_log_event(Process_Name,'Found Table to Import',Event_Date,Event_Time,Task_Start_Time,Task_End_Time, Task_Duration , 'Found Table to Import :' + file):\n",
    "   \n",
    "        \n",
    "read_and_import_all_csv_files('Y:/_Kaleida_Input/Available_Slots/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Table_SQL  = Create_Drop_Table_SQL('Access_DI')\n",
    "# SQL_Result = execute_SQL(Drop_Table_SQL)\n",
    "# out('SQL Execute Result: {} '.format(SQL_Result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8100f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting Iport Walk at Directory:',Data_Import_Starting_Directory)\n",
    "list_all_subdirectories(Data_Import_Starting_Directory)\n",
    "df_import_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374327f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "573b4eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import File = Y:\\_Kaleida_Input\\DailyAppointments\\July 2022.csv\n",
      "Header:  Index(['Appt Info', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4',\n",
      "       'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
      "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
      "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
      "       'Unnamed: 18', 'Patient Name', 'Account', 'DOB', 'Insurance',\n",
      "       'Primary Insurance', 'Primary Insurance ID', 'Secondary Insurance',\n",
      "       'Secondary Insurance ID', 'Tertiary Insurance', 'Tertiary Insurance ID',\n",
      "       'Patient Address 1', 'Patient Address 2', 'Zip Code', 'Email', 'Phone',\n",
      "       'Mobile', 'Age', 'Patient Status'],\n",
      "      dtype='object')\n",
      "row1_columns:     Appt Info  Unnamed: 1     Unnamed: 2   Unnamed: 3 Unnamed: 4 Unnamed: 5  \\\n",
      "0  Appt Date  Month Name  Week Day Name  Appt Length  Appt Time       Type   \n",
      "\n",
      "  Unnamed: 6   Unnamed: 7   Unnamed: 8   Unnamed: 9  ... Tertiary Insurance  \\\n",
      "0     Status  Full Status  Create Date  Cancel Date  ...                NaN   \n",
      "\n",
      "  Tertiary Insurance ID Patient Address 1 Patient Address 2 Zip Code Email  \\\n",
      "0                   NaN               NaN               NaN      NaN   NaN   \n",
      "\n",
      "  Phone Mobile Age Patient Status  \n",
      "0   NaN    NaN NaN            NaN  \n",
      "\n",
      "[1 rows x 37 columns]\n",
      "\n",
      " Historical TableCreate Table [pbic_1_0].[Daily_Appointments_DI](Appt Info nvarchar(255) , Unnamed: 1 nvarchar(255) , Unnamed: 2 nvarchar(255) , Unnamed: 3 nvarchar(255) , Unnamed: 4 nvarchar(255) , Unnamed: 5 nvarchar(255) , Unnamed: 6 nvarchar(255) , Unnamed: 7 nvarchar(255) , Unnamed: 8 nvarchar(255) , Unnamed: 9 nvarchar(255) , Unnamed: 10 nvarchar(255) , Unnamed: 11 nvarchar(255) , Unnamed: 12 nvarchar(255) , Unnamed: 13 nvarchar(255) , Unnamed: 14 nvarchar(255) , Unnamed: 15 nvarchar(255) , Unnamed: 16 nvarchar(255) , Unnamed: 17 nvarchar(255) , Unnamed: 18 nvarchar(255) , Patient Name nvarchar(255) , Account nvarchar(255)  , DOB nvarchar(255) , Insurance nvarchar(255) , Primary Insurance nvarchar(255) , Primary Insurance ID nvarchar(255) , Secondary Insurance nvarchar(255)  , Secondary Insurance ID nvarchar(255)  , Tertiary Insurance nvarchar(255)  , Tertiary Insurance ID nvarchar(255)  , Patient Address 1 nvarchar(255) , Patient Address 2 nvarchar(255) , Zip Code nvarchar(255) , Email nvarchar(255) , Phone nvarchar(255) , Mobile nvarchar(255) , Age nvarchar(255)  , Patient Status nvarchar(255) ); \n"
     ]
    }
   ],
   "source": [
    "def build_table_create_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # example = 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "        \n",
    "        \n",
    "    column_str =  = { \" \":\"_\",\"#\":\"Number\",\"%\":\"Percentage\" \\\n",
    "                         ,'_Unnamed':'','_Level':'',\"$\":\"Dollar\",'_1':'' \\\n",
    "                         ,'_2':'','_3':'','_4':'','_5':''  \\\n",
    "                         ,'_6':'','_7':'','_8':'','_9':''  \\\n",
    "                         ,'_0':'',':7':'',':8':'',':':'' }        \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "\n",
    "        \n",
    "        \n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1df22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Files to Import from Directory: Y:/_Kaleida_Input/ADP\\Daily Time Card/\n",
      "Creating_Table Time_Card  based upon 1st sample File 1 :  DailyTimecardReport.050421190755.csv\n",
      "raw file name to rcreate from: Y:/_Kaleida_Input/ADP\\Daily Time Card/Time_Card.csv\n",
      "Import File = DailyTimecardReport.050421190755.csv\n",
      "Header:  Index(['Payroll Company Code', 'Position ID', 'Payroll Name',\n",
      "       'Business Unit Description', 'Home Department Description',\n",
      "       'Location Description', 'Timecard Pay Date', 'REGULAR', 'OVERTIME',\n",
      "       'PTO-unscheduled', 'PTO', 'PTO BANK', 'PTO-UNSCHEDULED', 'REGSAL',\n",
      "       'UNPAID LEAVE', 'HOLIDAY', 'JURY', 'BEREAV', 'FLOAT', 'CME', 'FMLA',\n",
      "       'SICK', 'CSL', 'PFL', 'OTHER', 'TOV', '90D', 'OTH'],\n",
      "      dtype='object')\n",
      "row1_columns:    Payroll Company Code Position ID              Payroll Name  \\\n",
      "0                  G4H   G4H002752  Abdel-Aal Ahmed, Mohamed   \n",
      "\n",
      "  Business Unit Description  Home Department Description Location Description  \\\n",
      "0                Cardiology  CV_BGH B2_OUTPATIENT CLINIC            CV_BGH B2   \n",
      "\n",
      "  Timecard Pay Date  REGULAR  OVERTIME  PTO-unscheduled  ...  FLOAT  CME  \\\n",
      "0        09/07/2020      0.0       0.0              0.0  ...    0.0  0.0   \n",
      "\n",
      "   FMLA  SICK  CSL  PFL  OTHER  TOV  90D  OTH  \n",
      "0   0.0   0.0  0.0  0.0    0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1 rows x 28 columns]\n",
      "/n/nDROP_table_SQL: \n",
      "DROP Table [pbic_1_0].[Time_Card_DI]\n",
      "/n/ncreate_table_SQL: \n",
      "Create Table [pbic_1_0].[Time_Card_DI](Payroll Company Code nvarchar(255) , Position ID nvarchar(255) , Payroll Name nvarchar(255) , Business Unit Description nvarchar(255) , Home Department Description nvarchar(255) , Location Description nvarchar(255) , Timecard Pay Date nvarchar(255) , REGULAR nvarchar(255)  , OVERTIME nvarchar(255)  , PTO-unscheduled nvarchar(255)  , PTO nvarchar(255)  , PTO BANK nvarchar(255)  , PTO-UNSCHEDULED nvarchar(255)  , REGSAL nvarchar(255)  , UNPAID LEAVE nvarchar(255)  , HOLIDAY nvarchar(255)  , JURY nvarchar(255)  , BEREAV nvarchar(255)  , FLOAT nvarchar(255)  , CME nvarchar(255)  , FMLA nvarchar(255)  , SICK nvarchar(255)  , CSL nvarchar(255)  , PFL nvarchar(255)  , OTHER nvarchar(255)  , TOV nvarchar(255)  , 90D nvarchar(255)  , OTH nvarchar(255)  ); \n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def execute_table_Create_SQL(path, table_name):\n",
    "    \n",
    "    executing_SQL = False \n",
    "    \n",
    "    extension = 'csv'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        if csv_file_count == 1:\n",
    "            print('Creating_Table',table_name,' based upon 1st sample','File',str(csv_file_count),\": \", file)     \n",
    "            data_folder =  path\n",
    "            filename = data_folder + table_name +'.csv'\n",
    "            print ('raw file name to rcreate from:',filename)\n",
    "            DROP_table_SQL = build_DROP_table_SQL(file, table_name,'_DI]')  \n",
    "            create_table_SQL = build_table_create_SQL(file, table_name,'_DI]')\n",
    "            \n",
    "            print('/n/nDROP_table_SQL: ')  \n",
    "            print(DROP_table_SQL)  \n",
    "            print('/n/ncreate_table_SQL: ')              \n",
    "            print(create_table_SQL)\n",
    "\n",
    "#     if executing_SQL:     \n",
    "#             execute_SQL(DROP_table_SQL)            \n",
    "#             execute_SQL(create_table_SQL)\n",
    "        \n",
    "execute_table_Create_SQL('Y:/_Kaleida_Input/ADP\\Daily Time Card/','Time_Card') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def walk_sub_directories(root_directory): \n",
    " #print('list_sub_directories for root Ditrectory {} \\n'.format(root_directory) )   \n",
    " directory_entry = 0 \n",
    " Table_Name = ''\n",
    " print('About to Walk')\n",
    " for root,directory, file in os.walk(root_directory):\n",
    "    print('Walking....   ')\n",
    "    #print('Root Directory: {} subdir: {} \\n'.format(root_directory,directory_entry) )\n",
    "    \n",
    "    if root.find('Access') >= 0:\n",
    "        Table_Name = 'Access'\n",
    "    elif root.find('Daily Time Card') >= 0:\n",
    "        Table_Name = 'Daily_Time_Card'                \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'               \n",
    "    elif root.find('ADP') >= 0:\n",
    "        Table_Name = 'ADP'  \n",
    "    elif root.find('Employee Census') >= 0:\n",
    "        Table_Name = 'Employee_Census'   \n",
    "    elif root.find('Available_Slots') >= 0:\n",
    "        Table_Name = 'Available_Slots'     \n",
    "    elif root.find('Available_Slots_Past') >= 0:\n",
    "        Table_Name = 'Available_Slots_Past'    \n",
    "    elif root.find('Call Center') >= 0:\n",
    "        Table_Name = 'Call_Center'    \n",
    "    elif root.find('CPT Visit') >= 0:\n",
    "        Table_Name = 'CPT_Visit'    \n",
    "    elif root.find('Visit') >= 0:\n",
    "        Table_Name = 'Visit'  \n",
    "    elif root.find('DailyAppointments') >= 0:\n",
    "        Table_Name = 'Daily_Appointments'    \n",
    "    elif root.find('DailyCPT') >= 0:\n",
    "        Table_Name = 'Daily_CPT'    \n",
    "    elif root.find('DailyMultipleAppointmentSameDay') >= 0:\n",
    "        Table_Name = 'Daily_Multiple_Appointment_Same_Day'   \n",
    "    elif root.find('DailyScheduledOfficeAppointmentVisit') >= 0:\n",
    "        Table_Name = 'Daily_Scheduled_Office_Appointment_Visit'               \n",
    "        PatientExperienceDefault\n",
    "    else:\n",
    "        Table_Name = 'Unknown Table Name'\n",
    "    print('Root Directory'+str(directory_entry) +':',root+'Default Table Name for Directory :', Table_Name,' \\n')            \n",
    "    #list_all_csv_files(root)  \n",
    "    if Table_Name != 'Unknown Table Name':\n",
    "        execute_table_Create_SQL(root,Table_Name)  \n",
    "    directory_entry += 1     \n",
    " \n",
    "# Test function call     \n",
    "list_sub_directories('Y:/_Kaleida_Input/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import_directories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02725e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP the table Dynamically \n",
    "def drop_table_SQL(drop_SQL):\n",
    "    global sql_connector\n",
    "    print('Drop Table - Before SQL Connect - Call')\n",
    " #   logging.debug('Drop Table - Before SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(drop_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SQL  Dynamically \n",
    "def execute_SQL(execute_SQL_command):\n",
    "    global sql_connector\n",
    "    print('Execute SQL Connect - Call')\n",
    "    cnxn = pyodbc.connect(sql_connector)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(execute_SQL_command)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    " #   logging.debug('Drop Table - After SQL Connect - Call')    \n",
    "    \n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4db1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8df0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Drop_Table_SQL(Table_Name):\n",
    "    #drop_SQL = 'DROP TABLE [pbic_1_0].[' + Table_Name + ']'\n",
    "    \n",
    "    \n",
    "    drop_SQL =  'DROP TABLE [pbic_1_0].[{}]'.format(\"'\", Table_Name)\n",
    " \n",
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Access_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Access_DI]    \n",
    "    return drop_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop_Table_SQL = Create_Drop_Table_SQL('Access_DI')\n",
    "\n",
    "print(Drop_Table_SQL) \n",
    "#execute_SQL(Drop_Table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24728298",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1349a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_file(data_folder, filename):\n",
    "\n",
    "    print('Import File =', filename)                 \n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    #print(df_input_csv.columns)\n",
    "    df_input_csv.columns = df_input_csv.columns.map('^'.join)\n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"^\",\"\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\").replace(\"%\",\"Percentage\") \\\n",
    "                             .replace('_Unnamed','').replace('Unnamed','').replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "\n",
    "    print(df_input_csv.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430368e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Dataframe_table_create_SQL(dataframe_to_Create, table_name, table_Postfix  ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    column_str = '' \n",
    " \n",
    "\n",
    "    df_input_csv = pd.read_csv(filename, nrows=10)\n",
    "    number_of_columns = df_input_csv.shape[1]\n",
    "    header_columns = df_input_csv.columns\n",
    "    row1_columns = df_input_csv.iloc[0:1, : ]\n",
    "    print('Header: ',header_columns)\n",
    "    print('row1_columns: ',row1_columns)    \n",
    "\n",
    "    \n",
    "    df_input_csv.columns  = [x.strip().title().replace(\"Address 1\",\"Street_Address\").replace(\"Address 2\",\"Address_Two\") \\\n",
    "                             .replace(\"^\",\"\").replace(\"-\",\"_\").replace(\" \",\"_\").replace(\"#\",\"Number\").replace(\"#\",\"Number\") \\\n",
    "                             .replace(\"%\",\"Percentage\").replace('_Unnamed','').replace('Unnamed','') \\\n",
    "                             .replace('Unnamed:','').replace('_Level','').replace(\"$\",\"Dollar\") \\\n",
    "                             .replace('_1','').replace('_2','').replace('_3','').replace('_4','').replace('_5','')  \\\n",
    "                             .replace('_6','').replace('_7','').replace('_8','').replace('_9','')  \\\n",
    "                             .replace('1','').replace('2','').replace('3','').replace('4','').replace('5','')  \\\n",
    "                             .replace('6','').replace('7','').replace('8','').replace('9','').replace('0','')  \\\n",
    "                             .replace('_0','').replace(':7','').replace(':8','').replace(':','').replace('Unnamed: ','')  \\\n",
    "                             for x in df_input_csv.columns]\n",
    "    \n",
    "         \n",
    "            \n",
    "    #column_list = [x.strip().title().replace(\"^\",\"\") for x in df_input_csv.columns]\n",
    "    for col in range(0,number_of_columns):\n",
    "        #column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ' NULL, ' \n",
    "        column_str = column_str + str(df_input_csv.columns[col]) + ' ' + str(df_input_csv.dtypes[col]) + ', ' \n",
    "    if table_Postfix == '_DI]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"nvarchar(255)  \").replace(\"int64\",\"nvarchar(255)  \")\n",
    "    if table_Postfix == '_HX]':\n",
    "        column_str = column_str.replace(\"object\",\"nvarchar(255) \").replace(\"float64\",\"float  \").replace(\"int64\",\"int  \")\n",
    "        \n",
    "    Create_table_SQL  = 'Create Table ' + Table_Schema_Prefix + Table_Name + \"(\"  + column_str + \"); \"\n",
    "    Create_table_SQL = Create_table_SQL.replace(\", );\",\");\") \n",
    "    return Create_table_SQL\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "print(\"\\n Historical Table\" + create_table_SQL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07ed257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_DROP_table_SQL(filename , table_name, table_Postfix ):\n",
    "    Table_Schema_Prefix = '[pbic_1_0].[' \n",
    "    Table_Name = table_name        # 'Daily_Appointments'\n",
    "    Table_Name_Postfix = table_Postfix  # Passed in as '_DI]' or '_HX]'\n",
    "    Table_Name = Table_Name + Table_Name_Postfix\n",
    "    DROP_table_SQL  = 'DROP Table ' + Table_Schema_Prefix + Table_Name  \n",
    "    return DROP_table_SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_DI]')\n",
    "\n",
    "print(\"\\nDaily Table Create SQL: \\n\" + create_table_SQL)\n",
    "\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments','_HX]')\n",
    "\n",
    "print(\"\\nHistorical Table Create SQL: \\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y:/_Kaleida_Input/Access/2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459652_467_20220313074723_dmhmreport_EHRSUPPORT_2179577.csv'\n",
    "create_table_SQL = build_table_create_SQL(filename, 'Daily_Appointments')\n",
    "\n",
    "print(\"\\n\" + create_table_SQL)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF  EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[pbic_1_0].[Daily_Appointments_DI]') AND type in (N'U'))\n",
    "# DROP TABLE [pbic_1_0].[Daily_Appointments_DI]\n",
    "\n",
    "\n",
    "drop_SQL = 'DROP TABLE [pbic_1_0].[Daily_Appointments_DI]'\n",
    "drop_table_SQL(drop_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab349783",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_SQL(create_table_SQL)\n",
    "print(\"\\n\" + create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "# filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "data_folder =  Path('Y:\\_Kaleida_Input\\DailyAppointments')\n",
    "filename = data_folder / 'July 2022.csv'\n",
    " \n",
    "read_and_clean_file(data_folder,filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a1002",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_table_headers(input_data_frame):\n",
    "    global column_inserts\n",
    "    global column_question_mark\n",
    "    global create_table_SQL\n",
    "    global create_real_table_SQL\n",
    "    global insert_records_SQL\n",
    "    global Table_Name_Prefix\n",
    "    global Table_Name_Extension_Daily\n",
    "    global Table_Name_Extension_Historical\n",
    "    global create_schema_SQL\n",
    "    global create_real_schema_SQL\n",
    "        \n",
    "    Table_Name = 'Access'\n",
    "    sample_row = 3\n",
    "    create_table_SQL = ''\n",
    "    create_real_table_SQL = ''\n",
    "    insert_records_SQL = ''\n",
    "    Table_Name_Daily = Table_Name_Prefix + Table_Name + Table_Name_Extension_Daily\n",
    "    Table_Name_Historical = Table_Name_Prefix + Table_Name + Table_Name_Extension_Historical\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes\n",
    "    col_number = 0\n",
    "    column_creates = ''\n",
    "    column_values = ''\n",
    "    column_inserts = ''\n",
    "    real_column_creates = ''\n",
    "    column_question_mark = ''\n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "\n",
    "        if df_types[col_number-1] == 'object':\n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64': \n",
    "            real_sql_column_type = 'Varchar(255)'\n",
    "        else:\n",
    "            real_sql_column_type = 'Varchar(255)'                \n",
    "                \n",
    "\n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name\n",
    "        column_value = str(input_data_frame.iloc[sample_row,col_number-1])\n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type\n",
    "        real_column_creates = real_column_creates + column_name + \" \"  + real_sql_column_type\n",
    "        column_values = column_values + \"'\" + column_value + \"'\"\n",
    "        print(col_number, '  ', column_name)\n",
    "            \n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name_Daily + '  (' + column_inserts + ') + VALUES (' + column_values + '); '\n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name_Daily + '  (' + column_creates + '); '\n",
    "    create_real_table_SQL = 'CREATE TABLE ' + Table_Name_Historical + '  (' + real_column_creates + '); '\n",
    "    create_schema_SQL = create_schema_SQL + create_table_SQL\n",
    "    create_real_schema_SQL = create_real_schema_SQL + create_real_table_SQL\n",
    "    #logging.debug('Table Create Finished')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ee448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'Y:\\_Kaleida_Input\\Access\\2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "start_time1 = time.time()\n",
    "\n",
    "data_folder = Path('Y:\\_Kaleida_Input\\Access')\n",
    "filename = data_folder / '2459638_97_20220227013752_dmhmreport_EHRSUPPORT_5187581.csv'\n",
    "\n",
    "print('Import File =', filename)                 \n",
    "df_input_csv = pd.read_csv(filename, nrows=10, header=[0,1])\n",
    "df_input_csv.columns = df_input_csv.columns.map('_'.join)\n",
    "create_table_headers(df_input_csv) \n",
    "\n",
    "\n",
    "#print('\\n' + ' column_inserts:  ', column_inserts, '\\n') \n",
    "#print('\\n' + 'column_question_mark:  ', column_question_mark, '\\n') \n",
    "#print('\\n' + 'insert_records_SQL:  ', insert_records_SQL, '\\n')\n",
    "print('\\n' + 'create_table_SQL:  ', create_table_SQL, '\\n')\n",
    "print('\\n' + 'create_real_table_SQL:  ', create_real_table_SQL, '\\n')\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a36524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyappointment_df = pd.read_csv(r'C:\\DailyAppointment_A_J_Test\\Main 3.1 to 9.1.xlsx', low_memory = False, header = [0,1])\n",
    "Main 3.1 to 9.1.xlsx\n",
    "\n",
    "Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\n",
    "dailyappointment_df.columns = dailyappointment_df.columns.map('_'.join)\n",
    "dailyappointment_df = dailyappointment_df.fillna(0)\n",
    "dailyappointment_df['Unnamed: 3_Appt Length'] = dailyappointment_df['Unnamed: 3_Appt Length'].astype(int)\n",
    "#...\n",
    "engine = sqlalchemy.create_engine(\n",
    "               \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\",\n",
    "               echo=False)\n",
    "# # df = pd.read_sql_query('SELECT * FROM pbic_1_0.Access',conn)\n",
    "import time \n",
    "start_time1 = time.time()\n",
    "#dailyappointment_df.to_sql('dailyappointment_test', con=engine, if_exists='replace')\n",
    "end_time2 = time.time()\n",
    "print(f'{start_time1-end_time2:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time1 = time.time()\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 2.28 to 8.28.xlsx' )\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.2 to 9.2.xlsx' )\n",
    "\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub II 3.2 to 8.2.xlsx' )\n",
    "\n",
    "# Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 2.28 to 8.28.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.1 to 9.1.xlsx' )\n",
    "#Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Sub III 3.2.xlsx' )\n",
    "\n",
    "Available_Slots_df = pd.read_excel(r'Z:\\GPPC_SOURCE_FILES\\Oneday_data_04072022\\Oneday_data\\Available_Slots\\Main 3.1 to 9.1.xlsx' )\n",
    "Available_Slots_df.rename(columns={'Doctor Name' : 'Doctor_Name','Loc UId' : 'Loc_UId'}, inplace = True)\n",
    " \n",
    "shape = Available_Slots_df.shape\n",
    "print('\\nDataFrame Shape :', shape)\n",
    "print('\\nNumber of rows :', shape[0])\n",
    "print('\\nNumber of columns :', shape[1])\n",
    "\n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "file_read_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('Read raw file to Pandas Read Time',f'{file_read_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n",
    "create_table_headers(Available_Slots_df)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the Subfiles to iterate through \n",
    "def list_all_csv_files(path):\n",
    "    \n",
    "    extension = 'xlsx'\n",
    "    os.chdir(path)\n",
    "    print('CSV Files to Import from Directory:', path)\n",
    "    csv_file_count = 0\n",
    "    for file in glob.glob('*.{}'.format(extension)):\n",
    "        csv_file_count += 1 \n",
    "        print('File',str(csv_file_count),\": \", file)\n",
    "   \n",
    "        \n",
    "list_all_csv_files('Z:/GPPC_SOURCE_FILES/Oneday_data4_1_22/Available_Slots/')   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05638529",
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "\n",
    "Available_Slots_df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=20000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "\n",
    "\n",
    "# shape = Available_Slots_df.shape\n",
    "# print('\\nDataFrame Shape :', shape)\n",
    "# print('\\nNumber of rows :', shape[0])\n",
    "# print('\\nNumber of columns :', shape[1])\n",
    " \n",
    "# logging.debug('Table Create Finished')\n",
    "end_time2 = time.time()\n",
    "# row_count = Available_Slots_df.shape[1]\n",
    "execute_time = end_time2-start_time1\n",
    "print(' Rows Count:{}'.format(row_count) )\n",
    "\n",
    "print('SQL Insert Execution Time',f'{execute_time :.5f}')\n",
    "print(' Rows per second:',str(row_count/execute_time) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76371f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_headers(Available_Slots_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b56861",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE [pbic_1_0].[Available_Slots](\n",
    "\t[Date] [nvarchar](20) NULL,\n",
    "\t[Day] [nvarchar](50) NULL,\n",
    "\t[Time] [nvarchar](20) NULL,\n",
    "\t[Length] [int] NULL,\n",
    "\t[Dr] [nvarchar](50) NULL,\n",
    "\t[Doctor_Name] [nvarchar](max) NULL,\n",
    "\t[Loc] [nvarchar](50) NULL,\n",
    "\t[Loc_UId] [nvarchar](50) NULL,\n",
    "\t[Type] [nvarchar](50) NULL\n",
    ") ON [PRIMARY] TEXTIMAGE_ON [PRIMARY]\n",
    "GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70605344",
   "metadata": {},
   "outputs": [],
   "source": [
    "Available_Slots_df.info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Available_Slots', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21831e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert a row of values \n",
    "def insert_row_SQL(insert_row_SQL):\n",
    "    global server #= 'Kalpwvsqlgppc01' \n",
    "    global database #database = 'GPPC_DEV' \n",
    "    global username # =  'GPPC'\n",
    "    global pwd # ='Elephant-Trunk-06'\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=No;UID='+username+';PWD='+pwd)\n",
    "    cursor = cnxn.cursor()\n",
    "    sql_execute_result = cursor.execute(insert_row_SQL)\n",
    "    print('After SQL Call','Result Code: ',sql_execute_result)\n",
    "    cnxn.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea4d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[Access_DI]'\n",
    "drop_table_SQL(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_table_SQL = 'DROP TABLE [pbic_1_0].[dailyappointment_test]'\n",
    "drop_SQL_table(drop_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01eb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "constring = \"mssql+pyodbc://gppc:Elephant-Trunk-06@Kalpwvsqlgppc01/GPPC_DEV?DRIVER={ODBC Driver 17 for SQL Server}\"  \n",
    "engine = sqlalchemy.create_engine(constring,fast_executemany=True,echo=False)\n",
    "\n",
    "df.to_sql('Hx', con=engine, if_exists=\"append\",index=False,chunksize=1000, dtype = \n",
    "{'datefld': sqlalchemy.DateTime(), \n",
    "'intfld': sqlalchemy.types.INTEGER(),\n",
    "'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "'booleanfld': sqlalchemy.types.Boolean,\n",
    "'bool' : sqlalchemy.types.Boolean,\n",
    "'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "'int64' : sqlalchemy.types.INTEGER(),\n",
    "'object' : sqlalchemy.types.NVARCHAR(length=50000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global Process_Name \n",
    "\n",
    "today = date.today()\n",
    "\n",
    "start_time1 = time.time() \n",
    "time.sleep(3)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Starting Import Process',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Starting Import Process\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Reading the CSV files',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Reading the CSV filesS\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(1)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Writing to SQL Server',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Writing to SQL Server\")\n",
    "start_time1 = time.time() \n",
    "time.sleep(2)\n",
    "end_time2 = time.time() \n",
    "execute_time = end_time2-start_time1\n",
    "add_log_event(Process_Name,'Import Process END ',date.today(),datetime.now(),start_time1,end_time2, execute_time , \"Import Process END\")\n",
    "\n",
    "df_e_log.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e95b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "print('Insert SQL: ', insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rows(insert_SQL):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d62078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the parent directory for all the data subdirectories \n",
    "parent_dir = 'Y:/_Kaleida_Input/' #path to folder that contians the data folders\n",
    "\n",
    "path = parent_dir\n",
    "import_file_type = '\\*.csv'\n",
    "create_table_SQL = ''\n",
    "insert_records_SQL = '' \n",
    "create_schema_SQL = '' \n",
    "column_inserts  = ''\n",
    "column_question_mark  = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc27d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_import_files(data_directory_path, import_file_type ):\n",
    "    all_files = glob.glob(data_directory_path + import_file_type)\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4396de",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_SQL = create_insert_row(df_input_csv, 3, 'Access_DI')\n",
    "   ...: print('Insert SQL: ', insert_SQL)\n",
    "   ...: insert_row_SQL(insert_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca44715",
   "metadata": {},
   "outputs": [],
   "source": [
    " create_SQL_table(create_table_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_headers(input_data_frame):\n",
    "    global column_inserts  \n",
    "    global column_question_mark  \n",
    "    global create_table_SQL \n",
    "    global insert_records_SQL \n",
    "\n",
    "    Table_Name = 'Access'\n",
    "    df_cols = input_data_frame.columns\n",
    "    df_types = input_data_frame.dtypes \n",
    "    col_number = 0 \n",
    "    column_inserts = '' \n",
    "    column_creates = '' \n",
    "    column_question_mark = '' \n",
    "    for column_name in df_cols:\n",
    "        col_number = col_number + 1\n",
    "        if len(column_inserts) > 1:\n",
    "            column_inserts = column_inserts + \", \"\n",
    "        if len(column_creates) > 1:\n",
    "            column_creates = column_creates + \", \"     \n",
    "        if df_types[col_number-1] == 'object':\n",
    "            sql_column_type = 'Varchar(255)'\n",
    "        elif df_types[col_number-1] == 'float64':  \n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        else:\n",
    "            sql_column_type = 'Varchar(255)' \n",
    "        column_name = column_name.title()\n",
    "        column_name = column_name.replace(' ','_')\n",
    "        column_name = column_name.replace('#','Number')\n",
    "        column_inserts = column_inserts + column_name \n",
    "        \n",
    "        column_creates = column_creates + column_name + \" \"  + sql_column_type \n",
    "        column_question_mark = column_question_mark + \"?, \"\n",
    "        print(col_number, '  ', column_name) \n",
    "    #print('column_inserts:  ', column_inserts) \n",
    "    #print('column_question_mark:  ', column_question_mark) \n",
    "    print('column_creates:  ', column_creates)  \n",
    "\n",
    "    insert_records_SQL = 'INSERT INTO ' + Table_Name + '(' + column_inserts + ') VALUES (' + column_question_mark + ');' \n",
    "    create_table_SQL = 'CREATE TABLE ' + Table_Name + '(' + column_creates + ');' \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aebd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%who str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = iterate_import_files('Y:\\_Kaleida_Input\\Access','\\\\*.csv')\n",
    "# print(all_files[1])\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, nrows=10)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73677bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('C:\\Data\\Behavioral Health'+ '\\*.csv')\n",
    "print(all_files)\n",
    "print(all_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a38c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:41:00.444921Z",
     "start_time": "2022-08-30T12:40:48.868620Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove all csv files from dir and unzip folder\n",
    "parent_dir = 'C:/Power BI/' #path to folder\n",
    "path = parent_dir\n",
    "\n",
    "#get csv list\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        os.remove(path+file)    \n",
    "        \n",
    "with ZipFile(path+'PowerBiDownload.zip', 'r') as zipObj:\n",
    "   zipObj.extractall(path)   \n",
    "\n",
    "#remove files\n",
    "#remove files not in list\n",
    "csv_path = r'S:\\Data Team\\Source Data\\python sql\\needed tables.csv'\n",
    "ext = \".csv\"\n",
    "with open(csv_path, 'r') as csvfile:\n",
    "    good_files = []\n",
    "    for n in csv.reader(csvfile):\n",
    "        if len(n) > 0: good_files.append(n[0])\n",
    "    all_files = os.listdir(path)\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(ext) and filename not in good_files:\n",
    "            full_file_path = os.path.join(path, filename)\n",
    "            os.remove(full_file_path)\n",
    "\n",
    "print('Old files removed, new files unzipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6331",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T12:44:43.698446Z",
     "start_time": "2022-08-30T12:41:04.155886Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pull in helper tables, covert to csv and delete old helper tables\n",
    "with open('S:/Data Team/Source Data/python sql/helper tables.csv', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    linereader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in linereader:\n",
    "        name = row[0]\n",
    "        shutil.copy(name, 'C:\\Power BI\\\\' + os.path.basename(name))\n",
    "                \n",
    "print('All Helper Tables Moved')\n",
    "\n",
    "searchdir = 'C:\\Power BI\\\\'\n",
    "\n",
    "for xls_file in glob.glob(os.path.join(searchdir,\"*.xlsx\")):\n",
    "    data_xls = pd.read_excel(xls_file, index_col = None)\n",
    "    csv_file = os.path.splitext(xls_file)[0]+\".csv\"\n",
    "    data_xls.to_csv(csv_file, encoding = 'utf-8', index = False)\n",
    "    \n",
    "print('All Helper Tables Changed to CSV')\n",
    "\n",
    "df_pipe = pd.read_csv('C:/Power BI/hec daily.txt', delimiter = '|', index_col = None, header = None, on_bad_lines='skip')\n",
    "\n",
    "df_pipe.to_csv('C:/Power BI/hec daily.csv', sep = ',', header = False, index = False)    \n",
    "    \n",
    "print('HeC Daily Converted to CSV')\n",
    "\n",
    "pathtodelete =r\"C:\\Power BI\"\n",
    "filenames_xlsx = glob.glob(pathtodelete + \"/*.xlsx\")\n",
    "for i in filenames_xlsx:\n",
    "    os.remove(i)\n",
    "    \n",
    "filenames_txt = glob.glob(pathtodelete + \"/*.txt\")\n",
    "for k in filenames_txt:\n",
    "    os.remove(k)    \n",
    "    \n",
    "print('Old Helper Tables Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5e972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:27:48.939756Z",
     "start_time": "2022-08-30T13:27:35.810299Z"
    }
   },
   "outputs": [],
   "source": [
    "#rename files longer than >=63 char\n",
    "for filename in os.listdir(path):\n",
    "    if len(filename) > 63:\n",
    "        os.rename(path+filename, path+filename[-60:])\n",
    "        print(filename+' renamed to '+filename[-60:])\n",
    "            \n",
    "#get csv list\n",
    "csv_files = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)\n",
    "        \n",
    "data_path = path\n",
    "#create dataframes\n",
    "df = {}\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df[file] = pd.read_csv(data_path+file, low_memory=False, index_col=False)\n",
    "        \n",
    "    except UnicodeDecodeError:\n",
    "        df[file] = pd.read_csv(data_path+file, encoding=\"cp437\", low_memory=False, index_col=False, errors='ignore')\n",
    "    \n",
    "    print('Loading ' + file + ' into dataframe')    \n",
    "print('loading completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1bb416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-30T13:34:56.034850Z",
     "start_time": "2022-08-30T13:28:02.768562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in csv_files:\n",
    "    \n",
    "    dataframe = df[k]\n",
    "    \n",
    "    clean_tbl_name = k.lower().replace(\" \",\"_\").replace(\"-\",\"\").replace(\".\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\\\n",
    "    .replace(\"_csv\", \"\").replace(\"___\", \"_\").replace(\"__\", \"_\")    \n",
    "    \n",
    "    tbl_name = clean_tbl_name\n",
    "    \n",
    "    print(k + ' changing to ' + clean_tbl_name)\n",
    "\n",
    "    #clean column names\n",
    "    dataframe.columns = [x.lower().replace(\" \", \"_\").replace(\"-\", \"\").replace(\"#\",\"num\").replace(\"?\", \"\")\\\n",
    "                     .replace(\"=\",\"\").replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\r\\n\",\"\").replace(\"]\",\"_\")\\\n",
    "                     .replace(\"]\",\"_\").replace(\"[\",\"_\").replace(\"\\\\\",\"_\").replace(\".\",\"_\").replace(\"$\",\"\")\\\n",
    "                     .replace(\"%\",\"\").replace(\"#\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"?\",\"\")\\\n",
    "                     .replace(\",\",\"\").replace(\"*\",\"\").replace(\":\",\"\").replace(\"'\",\"\").replace(\"&\",\"\")\\\n",
    "                     .replace(\";\",\"\").replace(\"__\", \"_\").replace(\"/\", \"\")\n",
    "                     for x in dataframe.columns]\n",
    "\n",
    "     #limit column length to 64 and reading right to left\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "\n",
    "     #adding a number if duplicated column name\n",
    "    def uniquify(dataframe):\n",
    "        seen = set()\n",
    "\n",
    "        for item in dataframe:\n",
    "            fudge = 1\n",
    "            newitem = item\n",
    "\n",
    "            while newitem in seen:\n",
    "                fudge += 1\n",
    "                newitem = \"{}_{}\".format(item, fudge)\n",
    "\n",
    "            yield newitem\n",
    "            seen.add(newitem)\n",
    "\n",
    "    dataframe.columns = uniquify(dataframe)\n",
    "\n",
    "    dataframe.columns = dataframe.columns.str[-60:] \n",
    "    \n",
    "    #db settings and connection\n",
    "    #get password\n",
    "    f=open(\"S:/Data Team Secure/secrets/postgres.txt\",\"r\")\n",
    "    lines=f.readlines()\n",
    "    password=lines[1]\n",
    "    f.close()\n",
    "    \n",
    "   \n",
    "    user=\"Joes_User_Name\"\n",
    "    host = 'Joes_Host_Name'\n",
    "    dbname = 'postgres'\n",
    "        \n",
    "    engine = create_engine('postgresql://'+user+':'+password+'@'+host+'/'+dbname)\n",
    "   \n",
    "    #print('opened database successfully')\n",
    "    \n",
    "    #create table\n",
    "    #dataframe.to_sql(k, engine, schema = None, if_exists='append', index=False, dtype = 'text')\n",
    "    dataframe.to_sql(clean_tbl_name, engine, schema = None, if_exists='append', index=False, dtype =  \n",
    "                             {'datefld': sqlalchemy.DateTime(), \n",
    "                             'intfld':  sqlalchemy.types.INTEGER(),\n",
    "                             'strfld': sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'floatfld': sqlalchemy.types.Float(precision=3, asdecimal=True),\n",
    "                             'booleanfld': sqlalchemy.types.Boolean,\n",
    "                             'bool' : sqlalchemy.types.Boolean,\n",
    "                             'float64' : sqlalchemy.types.NVARCHAR(length=255),\n",
    "                             'int64' : sqlalchemy.types.INTEGER(),\n",
    "                             'object' : sqlalchemy.types.NVARCHAR(length=50000)})\n",
    "    \n",
    "    print(clean_tbl_name+' uploaded to database')\n",
    "    \n",
    "print('All uploads complete')\n",
    "\n",
    "## for automation, send email to email list when complete\n",
    "#get email and file list\n",
    "email_list = pd.read_csv('S:/Data Team/Source Data/python sql/email_db_upload.csv')\n",
    "emails = email_list['email']\n",
    "\n",
    "# email loop\n",
    "for i in range(len(emails)):\n",
    "    \n",
    "    email = emails[i]\n",
    "    \n",
    "    # Open the Outlook\n",
    "    outlook = win32.Dispatch('outlook.application')\n",
    "\n",
    "    # Create the email\n",
    "    mail = outlook.CreateItem(0)\n",
    "\n",
    "    # Set the email subject\n",
    "    mail.Subject = 'AUTOMATED EMAIL: Database Updated '+ datetime.now().strftime('%b %#d %Y %H:%M')\n",
    "\n",
    "    # Set the receiver email\n",
    "    mail.To = email\n",
    "\n",
    "    # Write the email content\n",
    "    mail.HTMLBody = r\"\"\"\n",
    "    <p>Hello</p>\n",
    "    <p>The database has had been updated successfully.</p>\n",
    "    <p>Thanks</p>\n",
    "    <p>The Data Team</p>\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the email\n",
    "    mail.Send()\n",
    "    print('Email sent to ' + email)\n",
    "print('All Emails Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e170cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'J:/OPA/GLIN Reporting Data Services Output/GPPC Scorecards/Sample report structure and colors.xlsx'\n",
    "\n",
    " \n",
    "File_Size = os.path.getsize(filepath)\n",
    "File_Last_Modified =  time.ctime(os.path.getmtime(filepath))\n",
    "File_Create_Date =  time.ctime(os.path.getctime(filepath))\n",
    "print('File Size ',File_Size)\n",
    "print('File Modified: ',File_Last_Modified)\n",
    "print('File Created: ',File_Create_Date)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
